# Producing weighted estimates
```{r setup6}
#| echo: false
#| message: false
#| warning: false

library(dplyr)
library(Hmisc)
library(ggplot2)
library(haven)
datadir<-"~/OneDrive/data/bsa/"
bsa<-read_spss(paste0(datadir,"8849spss_V1/bsa2017_open_enviropol.sav"))
bsa$RAgeCat.f<-droplevels(as_factor(bsa$RAgeCat))
bsa$Rsex.f<-droplevels(as_factor(bsa$Rsex))
```
Most users   of social surveys will be looking to  infer nationally representative estimates when conducting analyses. These entail using weights, which are meant to correct estimates for the under/over representation of certain groups in the sample due to the sampling process or  non response. Conducting sound inference relies not just on weighting estimates but also on computing adequate standard errors or conference intervals, which are measure of the precision of the estimates. 

The recommended approach to inferring confidence intervals and standard errors involves accounting for the survey design (ie the way  sampling was carried out) when conducting analyses -- which can be done with the `survey` package in R, a topic described in Section 6.3. At the same time, for users who are concerned with quickly computing reasonably accurate, rather than  publication-quality estimates, it may be useful to be aware which common R commands and operations can be used with weights. 

## Central tendency and dispersion (continuous variables)
  
The `Stats` packages which comes with the  installation of Base R includes `weighted.mean()` which, as indicated by its name, computes weighted estimates of the mean of a variable when weights are provided. However the `Hmisc` package includes a more comprehensive set of functions that can be used when weighting estimates. The code below provides an illustration of weighted means, variance and median of the left-right political attitudes score used in previous chapters, each time comparing it with the unweighted estimates:
  
```{r 6.1}
### Mean
c(mean(bsa$leftrigh,na.rm=T),
  wtd.mean(bsa$leftrigh,bsa$WtFactor)
  )

### Variance
c(var(bsa$leftrigh,na.rm=T),
  wtd.var(bsa$leftrigh,bsa$WtFactor))

### Median and quartiles
c(quantile(bsa$leftrigh,na.rm=T,probs=c(.25,.5,.75)),
  wtd.quantile(bsa$leftrigh,bsa$WtFactor,probs=c(.25,.5,.75)))

```

The above functions can be used in conjunction with `group_by()` and `summarise()` in order  to compute weighted estimates of continuous variables by groups of categorical variables:
  
```{r 6.2}
bsa%>%
  filter(!is.na(RAgeCat))%>%
  group_by(RAgeCat.f)%>%
  summarise(Mean=wtd.mean(leftrigh,WtFactor),
            Var=wtd.var(leftrigh,WtFactor),
            Median=wtd.quantile(leftrigh,WtFactor,probs=c(.5))
            )
```

## Frequencies and contingency tables

Neither `ftable()` or `table()` that were used in previous chapter allow weights. And although the `Hmisc` packages includes the `wtd.table()` function for one-way frequency tables, we recommend using `xtabs()` as previously, as it it more versatile. Indeed, as variables used with `xtabs()` are specified on the right hand side of a formula:

```
> xtabs(~var1, data=mydata)
```
or 

```
> xtabs(~var1 + var2, data=mydata)
```
... The variable containing the weights is  passed to `xtabs()` by specifying its name on the left hand side of the equation (or the tilde `~` ) 

```
> xtabs(weights~var1 + var2, data=mydata)
```

Let's apply this technique to investigate respondents' agreement with the sentence: *People should be able to travel by plane as much as they like, even if this harms the environment* as recorded in the `plnenvt` variable.

```{r 6.3}
bsa$plnenvt.f<-as_factor(bsa$plnenvt) # Converts the original variable into a factor
 
## Unweighted vs weighted frequency tables
cbind(
  Unweighted=round(
    100*prop.table(
      xtabs(~plnenvt.f,bsa,
            drop.unused.levels = T)
        ),
    1),
  Weighted=round(
    100*prop.table(
      xtabs(WtFactor~plnenvt.f,bsa,
            drop.unused.levels = T)
      ),
    1)
)
```

 
Obtaining weighted contingency tables follow the same logic:

```{r 6.4}
## Unweighted vs weighted contingency tables
cbind(
  round(
    100*prop.table(
      xtabs(~plnenvt.f+Rsex.f,bsa,
            drop.unused.levels = T),
      1),
    1),
  round(
    100*prop.table(
      xtabs(WtFactor~plnenvt.f+Rsex.f,bsa,
            drop.unused.levels = T),
      1),
    1)
)
```

## Inference using survey procedures

The weighting procedures described above could be described as 'quick and dirty' in that they compute representative point estimates -- ie a single value. Computing the precision of survey data estimates -- for example via their standard error -- requires more than just adding weights to a command. Information about the survey design, its primary sampling units, strata and clusters is required so that robust standard errors, statistical tests and/or confidence interval can be computed. The `Survey` package was designed in order to deal with this set of issues. It provides functions for integrating survey design into R as well as computing common estimates. Its  most important features are described below. 

In order to  use survey fonctions one first needs to create a `svydesign` object, in essence a version of the data that incorporates the sample design information available, then  compute the required estimate using the `svydesign` object.

An common issue with survey datasets available in the UK is that  sampling information is often only available in secured version of the data, restricting its access to authorised users in a secure lab. Although it is sometimes possible to use  available variables to account for aspects of the sample design -- region as a strata in the case of stratified samples -- in most cases users are left with computing standard errors without sample design information, which amounts to assuming that the sample was drawn purely at random. Even if this is the case however, using  the `survey` package is recommended, as it provides a coherent framework for computing survey parameters.     


```{r 6.5}
library(survey) ### Loading the package in memory
bsa.design<-svydesign(ids =~1,
                      weights=~WtFactor,
                      data=bsa) 
```

The code above simply declares the survey design by creating the `bsa.design` object (the name is arbitrary). The `ids=` parameter is where  primary sampling units are declared, as well as any clustering information as a formula ie `~PSU+cluster2id...`. When PSU information is unavailable `ids` is given the value 1 or 0. A `strata=` and `fpc=` are available in order to declare the sampling strata and the variable used for finite population correction. None of these are available in the bsa dataset, and estimation commands will therefore rely on the assumption of simple random sampling.

We can now compute estimates similar estimates as in the previous sections. The code below provides the mean of the left vs right political orientation indicator, as well as its 95% confidence interval: 
  
```{r 6.6}
a<-svymean(~leftrigh,
        bsa.design,
        na.rm = T)### Computes the mean and its standard error...

confint(a) ### ... and confidence interval

```
And now for the median:
  
```{r 6.7,error=T}
#| error: true
svyquantile(~leftrigh,
            bsa.design,
            quantiles=.5,
            na.rm = T)
```

Frequency and contingency tables are computed using `svytable()`, whose syntax relies on formulas  similarly to  `xtabs()` in the previous chapter.

```{r 6.8}
### A frequency table...
round(100*
        prop.table(
          svytable(~RAgeCat.f,bsa.design)
        ),
      1)

### And a two-way contingency table:

round(100*
        prop.table(
          svytable(~RAgeCat.f+Rsex.f,bsa.design),
          1),
      1)
```
