{
  "hash": "085e5ca6198c9dc4ed6da53c30acad46",
  "result": {
    "engine": "knitr",
    "markdown": "# Statistical testing\n\n\n\n::: {.cell}\n\n:::\n\n\n\nThis section covers how to implement common statistical tests with survey data in R. A working knowledge of these tests and their theoretical assumptions is assumed.\n\n## Differences between means\n\nTwo common ways of conducting statistical testing with means in samples consist in testing  whether they are significantly different from 0 (one  sample t-test), or whether they differ between two  groups (two samples t test). In the latter case, one can further distinguish between independent samples (where means come from different groups), or paired  samples (when the same measure is taken at several point in time). Given that it is probably one of the most widely used statistical tests in social sciences, we will only cover the former here. Several R packages provide functions for conducting t tests. We will be using `t.test()`, provided by the `stats` package (R Base). \n\nSuppose we would like to test whether the libertarianism vs authoritarianism score `libauth` significantly differs between men and women using a t test. A two sided test is the default, with H_0 or the null hypothesis being that there is no differences between groups, and H_1 or the alternative hypotheis that the  group means do indeed differ. The test is specified with a formula with on the left hand side the quantity to be tested and on the right  hand side the grouping variable. \n\nOne sided tests can be conducted by specifying that the alternative hypothesis (H_1) is either **greater** or **less**. `t.test()` assumes that by default the variances are unequal. This can be changed with the `var.equal=T` option.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Testing for significant differences in liberal vs authoritarian score\nt.test(libauth~Rsex.f,\n       data=bsa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  libauth by Rsex.f\nt = 1.1622, df = 3011.9, p-value = 0.2452\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -0.02035161  0.07959393\nsample estimates:\n  mean in group Male mean in group Female \n            3.527793             3.498172 \n```\n\n\n:::\n:::\n\n\nNo significant differences (ie the difference in  `libauth` between men and women is not significantly different from zero)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Testing for whether men have a lower (ie more left-wing)  score\nt.test(leftrigh~Rsex.f,\n       data=bsa, \n       alternative=\"less\")      \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  leftrigh by Rsex.f\nt = -2.0687, df = 2858, p-value = 0.01933\nalternative hypothesis: true difference in means between group Male and group Female is less than 0\n95 percent confidence interval:\n        -Inf -0.01197607\nsample estimates:\n  mean in group Male mean in group Female \n            2.487564             2.546087 \n```\n\n\n:::\n:::\n\n\nMen have a significantly lower  score on the scale (at the .05 threshold)  and are therefore on average leaning more to the left than women.\n\n\n## Differences in variance\n\nAnother common significance test in social science is the **variance test** which consists of  testing whether the variances of the same variable across two groups  are equal. This is usually achieved by testing whether the ratio of the variance between the two groups is significantly different from zero. With the BSA data, this amounts to testing whether men and women are more homogenous with regard to their political views.\n\nThe syntax for the variance test `var.test()` also included in `stats` is almost identical to that of `t.test()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Testing for gender differences in liberal vs authoritarian score\nvar.test(libauth~Rsex.f,\n         data=bsa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tF test to compare two variances\n\ndata:  libauth by Rsex.f\nF = 1.0892, num df = 1434, denom df = 1777, p-value = 0.0879\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.9873927 1.2022204\nsample estimates:\nratio of variances \n          1.089239 \n```\n\n\n:::\n:::\n\n\nSignificant differences in the variance  between men and women, but only at the .1 threshold.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Testing for whether men have a lower (ie more left-wing)  score\nvar.test(leftrigh~Rsex.f,\n         data=bsa,\n         alternative=\"greater\")      \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tF test to compare two variances\n\ndata:  leftrigh by Rsex.f\nF = 1.3218, num df = 1433, denom df = 1771, p-value = 1.263e-08\nalternative hypothesis: true ratio of variances is greater than 1\n95 percent confidence interval:\n 1.217167      Inf\nsample estimates:\nratio of variances \n            1.3218 \n```\n\n\n:::\n:::\n\n\nThe variance of left-right political leaning is larger among men than women, in other words there are more divergence between men than between women.\n\n## Significance of measures of association\n\n**Between continuous variables**\n  \n  Another type of  common statistical   test in social science is about examining whether a coefficient of correlation is significantly different from 0 (alternative hypothesis).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(bsa$leftrigh, bsa$libauth, use='complete.obs')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  bsa$leftrigh and bsa$libauth\nt = 0.54472, df = 3202, p-value = 0.586\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.02501074  0.04423951\nsample estimates:\n        cor \n0.009625928 \n```\n\n\n:::\n:::\n\n\n\nAs we could have suspected the coefficient of correlation between the two scales is so small that it cannot be said to be  significantly  different from zero.\n\n**Between categorical variables**\n  \n  The chi-square test of independence is a very common  test of association between categorical variables. It consists in examining whether the association between two variables is likely to be due to chance or not, in other words whether the variability observed in a contingency table is significantly different from what would be expected were it due to chance.\n\nWe will be using `chisq.test()`, also  from the `stats` package. By contrast with the test of correlation, the `chisq.test()` needs to be applied to  contingency tables that have already been computed.\nLet us go back to an earlier example, and attempt to test whether the gender differences in political affiliations are due to chance or not. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(xtabs(~PartyId2 +Rsex.f,bsa))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  xtabs(~PartyId2 + Rsex.f, bsa)\nX-squared = 27.191, df = 5, p-value = 5.236e-05\n```\n\n\n:::\n:::\n\n\n\nAs the R output shows, there are highly significant gender differences in political affiliations (p<.001).\n\n\\newpage\n\n",
    "supporting": [
      "8_Testing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}