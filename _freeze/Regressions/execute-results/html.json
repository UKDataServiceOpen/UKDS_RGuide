{
  "hash": "b1a85dadf98673daa45d656f18493350",
  "result": {
    "markdown": "# Regression analysis\n\n::: {.cell hash='Regressions_cache/html/setup5_2ed9da44cbb74e09fbe9a7e1de14bf2f'}\n\n:::\n\nThe `glm()` command from the R base package is  used for fitting linear and non-linear models. These include logistic regression, and more generally models falling under the Generalized Linear Model framework.  \nIn this section, we will use it to investigate the association between the level of education `HEdQual3` and `Voted`, whether someone voted or not. Let’s first briefly explore the variables using the `class()` and `table()` commands from the previous chapters:\n  \n\n::: {.cell hash='Regressions_cache/html/9.1_e02c7eb645d821a7e0248413d57ba6f0'}\n\n```{.r .cell-code}\nclass(bsa$Voted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"factor\"\n```\n:::\n\n```{.r .cell-code}\ntable(bsa$Voted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n Yes   No \n2205  776 \n```\n:::\n:::\n\nand \n\n\n::: {.cell hash='Regressions_cache/html/9.2_99cd670440797d80798afd0b8ddf8804'}\n\n```{.r .cell-code}\nclass(bsa$HEdQual3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"factor\"\n```\n:::\n\n```{.r .cell-code}\ntable(bsa$HEdQual3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n                          Degree Higher educ below degree/A level \n                            1050                             1086 \n            O level or equiv/CSE                 No qualification \n                            1026                              747 \n```\n:::\n:::\n\n\nIt is a good idea to make sure that  categorical variables are stored as factors. This is not absolutely necessary, but gives greater flexibility, for instance when having to change the reference category on the go.\n\nFor greater readability we can also shorten the levels of `HEdQual3`  using the `level()` function:\n  \n\n::: {.cell hash='Regressions_cache/html/9.3_46c896e964bdece9dadc62ca9888e7fd'}\n\n```{.r .cell-code}\nlevels(bsa$HEdQual3) ### The original level names  are a bit verbose...\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Degree\"                           \"Higher educ below degree/A level\"\n[3] \"O level or equiv/CSE\"             \"No qualification\"                \n```\n:::\n\n```{.r .cell-code}\n### ... Fortunately we can shorten them easily\nlevels(bsa$HEdQual3) <- c(\"Degree\",\"A level and above\",\"GCSE or equiv\",\"No Qual\")\n\ntable(bsa$HEdQual3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n           Degree A level and above     GCSE or equiv           No Qual \n             1050              1086              1026               747 \n```\n:::\n:::\n\n\nWhat about the levels for our dependent variable? By default, the first level of a factor will be used as the reference category. This can be also checked with the `contrasts()`. In this case,  1 is associated with ‘No’, so the model will be predicting the probability of NOT voting. If the 1 was associated with ‘Yes’ then the model will be predicting the probability of voting.\n\n\n::: {.cell hash='Regressions_cache/html/9.4_34ec13abf3a2139d0783053b2e58611b'}\n\n```{.r .cell-code}\nlevels(bsa$Voted) #Note that Yes comes before No\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Yes\" \"No\" \n```\n:::\n\n```{.r .cell-code}\ncontrasts(bsa$Voted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    No\nYes  0\nNo   1\n```\n:::\n:::\n\n\nAs we are interested in the latter we need to change the reference category using the `relevel()` function. We can also create a new variable named `Voted2` so as to keep a copy of the original variable intact. \n\n\n::: {.cell hash='Regressions_cache/html/9.5_5d7533c146b36cf48eaf16096eb60e7b'}\n\n```{.r .cell-code}\n# Reverse the order\nbsa$Voted2 <- relevel(bsa$Voted, ref = \"No\")\n\n#Check the contrasts\ncontrasts(bsa$Voted2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Yes\nNo    0\nYes   1\n```\n:::\n:::\n\nSince the outcome  variable (Voted or Voted2) has a binomial distribution, we need to specify to the glm() function that we will be fitting a logistic regression model. We will do this by setting the argument 'family'  to 'binomial' and the link function to 'logit'. We could  also have used 'probit' instead as a link function.\nThe code below runs the model and stores the result into an object called `fit1`:\n  \n  \n\n::: {.cell hash='Regressions_cache/html/9.6_7101990bc2748227ccf01899dbe0788c'}\n\n```{.r .cell-code}\nfit1 <- glm(Voted2 ~ HEdQual3, data=bsa, family=binomial(link=logit))\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Voted2 ~ HEdQual3, family = binomial(link = logit), \n    data = bsa)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                1.49561    0.09188  16.278  < 2e-16 ***\nHEdQual3A level and above -0.21342    0.12514  -1.706   0.0881 .  \nHEdQual3GCSE or equiv     -0.64062    0.12191  -5.255 1.48e-07 ***\nHEdQual3No Qual           -0.83672    0.12769  -6.553 5.65e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3297.6  on 2916  degrees of freedom\nResidual deviance: 3240.4  on 2913  degrees of freedom\n  (1071 observations deleted due to missingness)\nAIC: 3248.4\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nTo run a model  controlling for gender ‘Rsex’ and age ‘RAgeCat’, one  simply needs to add them on the right hand side of the formula, separated with a plus (+) sign. \n\n\n::: {.cell hash='Regressions_cache/html/9.7_217891c5567493d7aa5a409dc6c8439a'}\n\n```{.r .cell-code}\nfit2 <- glm(Voted2 ~ HEdQual3 + Rsex + RAgeCat, data=bsa, family=binomial(link=logit))\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Voted2 ~ HEdQual3 + Rsex + RAgeCat, family = binomial(link = logit), \n    data = bsa)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                1.11251    0.20044   5.550 2.85e-08 ***\nHEdQual3A level and above -0.38676    0.13215  -2.927 0.003427 ** \nHEdQual3GCSE or equiv     -0.99023    0.13109  -7.554 4.23e-14 ***\nHEdQual3No Qual           -1.90625    0.15687 -12.152  < 2e-16 ***\nRsexFemale                -0.15708    0.09218  -1.704 0.088363 .  \nRAgeCat25-34              -0.24604    0.19670  -1.251 0.210996    \nRAgeCat35-44               0.20668    0.19808   1.043 0.296764    \nRAgeCat45-54               0.85685    0.20000   4.284 1.83e-05 ***\nRAgeCat55-59               0.84062    0.23225   3.619 0.000295 ***\nRAgeCat60-64               1.60276    0.25272   6.342 2.27e-10 ***\nRAgeCat65+                 2.16408    0.21450  10.089  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3293.1  on 2912  degrees of freedom\nResidual deviance: 2922.5  on 2902  degrees of freedom\n  (1075 observations deleted due to missingness)\nAIC: 2944.5\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\n**Model interpretation**\n  \n  `Summary()` provide a broad overview of the model output, not dissimilar to other statistical software. \nWe can also examine  the content of fit1 and fit2  more in detail and requests a specific element, for example:\n  \n\n::: {.cell hash='Regressions_cache/html/9.8_cf7104c17f08ecf1f8b2e88fdd8ae783'}\n\n```{.r .cell-code}\nls(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"aic\"               \"boundary\"          \"call\"             \n [4] \"coefficients\"      \"contrasts\"         \"control\"          \n [7] \"converged\"         \"data\"              \"deviance\"         \n[10] \"df.null\"           \"df.residual\"       \"effects\"          \n[13] \"family\"            \"fitted.values\"     \"formula\"          \n[16] \"iter\"              \"linear.predictors\" \"method\"           \n[19] \"model\"             \"na.action\"         \"null.deviance\"    \n[22] \"offset\"            \"prior.weights\"     \"qr\"               \n[25] \"R\"                 \"rank\"              \"residuals\"        \n[28] \"terms\"             \"weights\"           \"xlevels\"          \n[31] \"y\"                \n```\n:::\n\n```{.r .cell-code}\nround(fit1$coefficients,2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              (Intercept) HEdQual3A level and above     HEdQual3GCSE or equiv \n                     1.50                     -0.21                     -0.64 \n          HEdQual3No Qual \n                    -0.84 \n```\n:::\n\n```{.r .cell-code}\n### The coef() function will give the same output:\nround(coef(fit1),2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              (Intercept) HEdQual3A level and above     HEdQual3GCSE or equiv \n                     1.50                     -0.21                     -0.64 \n          HEdQual3No Qual \n                    -0.84 \n```\n:::\n:::\n\nIt is beyond the remit of this guide to describe the full output of `glm()`. Please refer to the package documentation for more detailed explanations.\n\nRaw logistic regression coefficients measure the effect of variables on the probability of the outcome such as log(betaX)=P(y). It is common practice to  convert these into odd ratios  by exponentiating them, such as that betaX=exp(P(y)). The following code does this in R: \n  \n\n::: {.cell hash='Regressions_cache/html/9.9_2d4aef354696fb0c9212c07fab7b909a'}\n\n```{.r .cell-code}\ncbind(\n  exp(coef(fit2)),exp(confint(fit2))\n) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                        2.5 %     97.5 %\n(Intercept)               3.0419750 2.0618311  4.5276614\nHEdQual3A level and above 0.6792550 0.5237628  0.8795335\nHEdQual3GCSE or equiv     0.3714919 0.2868078  0.4796010\nHEdQual3No Qual           0.1486366 0.1089442  0.2015607\nRsexFemale                0.8546343 0.7130680  1.0235515\nRAgeCat25-34              0.7818917 0.5300310  1.1469983\nRAgeCat35-44              1.2295882 0.8316608  1.8095457\nRAgeCat45-54              2.3557310 1.5886305  3.4827615\nRAgeCat55-59              2.3178122 1.4718049  3.6621685\nRAgeCat60-64              4.9667132 3.0428167  8.2090111\nRAgeCat65+                8.7065916 5.7183039 13.2696921\n```\n:::\n:::\n\nUsing the `coef()` and `confint()` functions, the code above respectively extracts the coefficients and associated 95% confidence intervals from fit2 then collate them using `cbind()`.\n\n** Plotting the coefficients ** -->\n  We can visualise the odd ratios and their confidence intervals using the `plot.model()` function from the ‘sjPlot’ package.\nThe ’sjPlot’ package needs to be installed and loaded\n\n```\ninstall.packages('sjPlot')\n```\n\n\n::: {.cell hash='Regressions_cache/html/9.10_20435bdccabbb4dcc2743b195cf731da'}\n\n```{.r .cell-code}\nlibrary(sjPlot)\nset_theme(base = theme_minimal()) ### Default sets of options \nplot_model(fit2,\n           colors = c(\"#702082\", \"#008755\") ### Added for better accessibility \n) \n```\n\n::: {.cell-output-display}\n![](Regressions_files/figure-html/9.10-1.png){fig-alt='Horizontal line plot for the odds ratios of the regression of voting behaviour by qualification, age categories and gender together with their confidence intervals' width=672}\n:::\n:::\n\n\n\n**Assessing model fit**\n  The  Akaike Information Criterion (AIC) is a measure of relative fit for  maximum likelihood fitted models. It is used to compare the improvement in how several models fit some data relative to each other, allowing for the different number of parameters or degrees of freedom. The smaller the AIC, the better the fit. In order for the comparison to be valid, we need to ensure that the models were run with the same number of observations each time. As it is likely that the second model was run on a smaller sample, due to missing values for the Age and Sex variables, we will need to re-run the first one without.\n\n\n\n::: {.cell hash='Regressions_cache/html/9.11_65103b8486e902f6cd671993c8b1149b'}\n\n```{.r .cell-code}\nfit1 <- glm(Voted2 ~ HEdQual3, data=bsa%>%\n              filter(!is.na(Rsex) & !is.na(RAgeCat)), family=binomial(link=logit))\n\nc(fit1$aic,fit2$aic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3244.507 2944.535\n```\n:::\n:::\n\nWe can see that the model controlling for gender and sex is a better fit to the data than the one without controls as it has an AIC of 2944.5 against 3244.5 for fit1.\n\nWith the information about the deviance from fit1 and fit2, we can also compute the overall significance of the model, that is whether the difference between the deviance (another  likelihood-based measure of fit) for the fitted model is significantly different from that of the empty or null model. This is usually carried by conducting a chi square test, accounting for the differences in the number of parameters (ie degrees of freedom) between the two models. As with other R code, this can achieved step by step or in one go:\n  \n\n::: {.cell hash='Regressions_cache/html/9.12_43e72e9308619ea761aba104c856f1c1'}\n\n```{.r .cell-code}\ndev.d<-fit2$null.deviance - fit2$deviance \ndf.d<-fit2$df.null - fit2$df.residual\np<-1 - pchisq(dev.d, df.d)\nc(dev.d,df.d,p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 370.5486  10.0000   0.0000\n```\n:::\n:::\n\nThe Chi square test indicates that the  difference in deviance of 370.5 with 10 degrees of freedom is highly significant (P<.001) \n\\newpage\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}