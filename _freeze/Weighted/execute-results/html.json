{
  "hash": "381b1271f5380978551258d354733b25",
  "result": {
    "markdown": "# Producing weighted estimates\n\n::: {.cell hash='Weighted_cache/html/setup5_58e91feeac1392982c66b6cbccbc54d9'}\n\n:::\n\nMost users   of social surveys are interested at some point in inferring nationally representative estimates and/or compensate for bias involved in the sampling process when conducting analyses: sampling and non-response bias. These are often tackled with sampling weights, which are meant to correct estimates for the under/over representation of certain groups in the sample and adjusts standard errors accordingly. \n\nHowever, robust inference usually relies not just on weighting estimates   but also on factoring in the survey design when conducting analyses -- which can be done with the `survey` package in R, but  is a topic that goes beyond the present guide. At the same time for users who are concerned with reducing bias rather than producing publication-quality estimates, it may be useful to be aware how common R commands and operations can be used with weights. \n\nSome of the most common ones are mentioned below:\n  \n  ## Central tendency and dispersion (continuous variables)\n  \n  The `stats` packages which comes with the  installation of Base R includes `weighted.mean()` which, as indicated by its name, computes weighted estimates of the mean of a variable when weights are provided. However the Hmisc package includes a more comprehensive set of functions that can be used when weighting estimates. The code below provides an illustration of weighted means, variance and median of the left-right score used before, each time comparing it with the unweighted estimate:\n  \n\n::: {.cell hash='Weighted_cache/html/6.1_04a47359f7ef8a9f912046220a442c01'}\n\n```{.r .cell-code}\n### Mean\nc(mean(bsa$leftrigh,na.rm=T),wtd.mean(bsa$leftrigh,bsa$WtFactor))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.519911 2.521589\n```\n:::\n\n```{.r .cell-code}\n### Variance\nc(var(bsa$leftrigh,na.rm=T),wtd.var(bsa$leftrigh,bsa$WtFactor))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6166894 0.6195378\n```\n:::\n\n```{.r .cell-code}\n### Median and quartiles\nc(quantile(bsa$leftrigh,na.rm=T,probs=c(.25,.5,.75)),\n  wtd.quantile(bsa$leftrigh,bsa$WtFactor,probs=c(.25,.5,.75)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n25% 50% 75% 25% 50% 75% \n2.0 2.4 3.0 2.0 2.4 3.0 \n```\n:::\n:::\n\n\nThe above functions can be used in conjunction with `group_by()` and `summarise()` in order  to compute weighted estimates of continuous variables by groups of categorical variables:\n  \n\n::: {.cell messages='false' hash='Weighted_cache/html/6.2_0ea38ee86fa4b4739314cd290921c15c'}\n\n```{.r .cell-code}\nbsa%>%\n  filter(!is.na(RAgeCat))%>%group_by(RAgeCat)%>%\n  summarise(Mean=wtd.mean(leftrigh,WtFactor),\n            Var=wtd.var(leftrigh,WtFactor),\n            Median=wtd.quantile(leftrigh,WtFactor,probs=c(.5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 Ã— 4\n  RAgeCat  Mean   Var Median\n  <fct>   <dbl> <dbl>  <dbl>\n1 18-24    2.49 0.556    2.4\n2 25-34    2.56 0.577    2.6\n3 35-44    2.52 0.615    2.4\n4 45-54    2.53 0.671    2.6\n5 55-59    2.54 0.653    2.4\n6 60-64    2.46 0.685    2.4\n7 65+      2.52 0.613    2.4\n```\n:::\n:::\n\n\n## Frequencies and contingency tables\n\nNeither `ftable()` or `table()` used above allow for using weights. And although the `Hmisc` packages includes the `wtd.table()` function for single frequency tables, we recommend using `xtabs()` as previously, as it it more versatile:\n  \n\n::: {.cell messages='false' hash='Weighted_cache/html/6.3_bce1f8e4ffd974ce8137804d635d0340'}\n\n```{.r .cell-code}\n## Unweighted vs weighted frequency tables\ncbind(Unweighted=round(100*prop.table(xtabs(~plnenvt,bsa)),1),\n      Weighted=round(100*prop.table(xtabs(WtFactor~plnenvt,bsa)),1)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           Unweighted Weighted\nagree strongly                    4.6      4.8\nagree                            16.0     15.9\nneither agree nor disagree       33.2     33.2\ndisagree                         36.3     37.0\ndisagree strongly                10.0      9.0\n```\n:::\n:::\n\n\nWeights are passed to `xtabs()` by specifying their name on the left hand side of the equation (or the tilde `~` ) \n\nObtaining weighted contingency tables follow the same logic:\n\n::: {.cell messages='false' hash='Weighted_cache/html/6.4_152e97a11eefedab00912825cd73259d'}\n\n```{.r .cell-code}\n## Unweighted vs weighted contingency tables\ncbind(round(100*prop.table(xtabs(~plnenvt+Rsex,bsa),1),1),\n      round(100*prop.table(xtabs(WtFactor~plnenvt+Rsex,bsa),1),1)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           Male Female Male Female\nagree strongly             50.0   50.0 46.1   53.9\nagree                      47.2   52.8 53.5   46.5\nneither agree nor disagree 40.8   59.2 43.6   56.4\ndisagree                   47.9   52.1 52.7   47.3\ndisagree strongly          42.3   57.7 41.5   58.5\n```\n:::\n:::\n\n\n## Robust inference\n\nThe weighting procedures described above could be described as 'quick and dirty' in that they mostly compute point estimates -- ie a single value -- and do not provide  a reliable idea of their precision. Computing the precision of survey data estimates -- usually via their standard error -- usually requires more than just adding weights to a command. Information about the survey design, its primary sampling units, strata and clusters is requires so that robust standard errors, statistical tests and/or confidence interval are computed.\n\nThe `Survey` package was designed in order to deal with this set of issues. It provides functions for integrating survey design into R as well as computing common estimates. We describe below the most important features. In order to  use survey fonctions consist one first needs to create a svydesign object, in essence a version of the data that incorporates the sample design information available, then to compute the estimate using the svydesign object.\n\nAn common issue with survey datasets available in the UK is that  sampling information is often only available in secure lab version of the data, restricting its access to authorised users. Although it is sometimes possible to use  available variables to account for aspects of the sample design -- region as a strata in the case of stratified samples -- in most cases users are left with computing standard errors without sample design information, which amounts to assuming that the sample was drawn purely at random. Even if this is the case however, using  the `survey` package is recommended, as it provides a coherent framework for computing survey parameters.     \n\n\n\n::: {.cell messages='false' hash='Weighted_cache/html/6.5_8c39f299b0370b949f20c5c8300db642'}\n\n```{.r .cell-code}\nlibrary(survey) ### Loading the package in memory\nbsa.design<-svydesign(ids =~1,weights=~WtFactor,data=bsa) \n```\n:::\n\n\nThe code above simply declares the survey design by creating the `bsa.design` object (the name is arbitrary). The `ids=` parameter is where  primary sampling units are declared, as well as any clustering information as a formula ie `~PSU+cluster2id...`. When PSU information is unavailable `ids` is given the value 1 or 0. A `strata=` and `fpc=` are available in order to declare the sampling strata and the variable used for finite population correction. None of these are available in the bsa dataset, and estimation commands will therefore rely on the assumption of simple random sampling.\n\nWe can now compute estimates similar estimates as in the previous sections. The code below provides the mean of the left vs right political orientation indicator, as well as its 95% confidence interval: \n  \n\n::: {.cell messages='false' hash='Weighted_cache/html/6.6_651ccb0a5531647cad6a1582c70e1d35'}\n\n```{.r .cell-code}\nsvymean(~leftrigh,bsa.design,na.rm = T)### Computes the mean and its standard error...\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           mean     SE\nleftrigh 2.5216 0.0155\n```\n:::\n\n```{.r .cell-code}\nconfint(svymean(~leftrigh,bsa.design,na.rm = T)) ### ... and confidence interval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            2.5 %   97.5 %\nleftrigh 2.491277 2.551902\n```\n:::\n:::\n\nAnd now for the median:\n  \n\n::: {.cell messages='false' hash='Weighted_cache/html/6.7_e36ffe0f559dac79deeb7f544a09d8ea'}\n\n```{.r .cell-code}\nsvyquantile(~leftrigh,bsa.design,quantiles=.5,na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$leftrigh\n    quantile ci.2.5 ci.97.5         se\n0.5      2.4    2.4     2.6 0.05100208\n\nattr(,\"hasci\")\n[1] TRUE\nattr(,\"class\")\n[1] \"newsvyquantile\"\n```\n:::\n:::\n\n\nFrequency and contingency tables are computed using `svytable()`, which follows the same syntax as `xtabs()`\n\n\n::: {.cell messages='false' hash='Weighted_cache/html/6.8_ae0e418f8e273ca0e53cc3e1ce725008'}\n\n```{.r .cell-code}\n### A frequency table...\nround(100*\n        prop.table(\n          svytable(~RAgeCat,bsa.design)\n        ),1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRAgeCat\n18-24 25-34 35-44 45-54 55-59 60-64   65+ \n 11.2  17.2  16.1  17.9   7.9   6.8  22.8 \n```\n:::\n\n```{.r .cell-code}\n### And a two-way contingency table:\n\nround(100*\n        prop.table(\n          svytable(~RAgeCat+Rsex,bsa.design)\n          ,1),1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Rsex\nRAgeCat Male Female\n  18-24 51.1   48.9\n  25-34 50.2   49.8\n  35-44 49.7   50.3\n  45-54 49.3   50.7\n  55-59 48.8   51.2\n  60-64 49.0   51.0\n  65+   45.4   54.6\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}