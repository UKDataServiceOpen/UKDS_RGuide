[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysing social survey data using R",
    "section": "",
    "text": "Introduction\nThis guide provides an introduction to analysing large scale social survey data using R, with examples from the British Social Attitudes Survey 2017. It is aimed at two categories of users: \n\nThose inside or outside higher education, or who do not have access to commercial statistical software such as Stata, SPSS or SAS but who would like to conduct their own analysis beyond what is usually published by data producers: for example statistics for specific groups of the population. This guide provides this group of users with a range of procedures that will help them produce straightforward and robust analyses tailored to their needs without spending unnecessary time on learning the inner workings of R. \nMore advanced users who are already familiar with other data analysis tools but who would like to learn how to carry out their analyses in R.\n\nA wealth of introductory content on R programming is already available online. The guide therefore focuses on providing succinct examples of common operations that most users of social surveys carry out in the course of their research, including how to:\n\nread in and open datasets.\ndo common data manipulation operations.\nproduce simple descriptive statistics or tabulations.\nhandle survey weights and survey design variables",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_Introduction.html",
    "href": "1_Introduction.html",
    "title": "1  What is R ?",
    "section": "",
    "text": "R is a free, user developed, object-oriented statistical programming language. It originates in the ‘S’ and ‘S Plus’ languages developed in the 1970s and 1980s. It has been widely used in science, statistics and data sciences and is increasingly being adopted in the social sciences for teaching and research purposes.\nR can be downloaded from the Comprehensive R Archive Network (CRAN) website. Installation instructions as well as guides, tutorials and FAQ are available on the CRAN website. A considerable amount of R-related resources is available online.\nAnyone can install and use R without charge, and to some extent contribute and amend the existing program itself. R is particularly favoured by users who want to develop their own statistical functions or implement technical advances that are not yet available in commercial packages. The existence of a vast number of user written packages (21964 at the time of writing this guide) is one of the strengths of R. Users interested in publishing their own packages on CRAN should be aware that a minimum set of rules need to be followed by contributors.\nAlthough R can perform most of the analyses available in traditional statistical software such as Stata, SPSS, or SAS, it has broader applications used for mapping, data mining or machine learning. Its flexibility as a language allows users to carry out analyses in multiple ways, each with distinct advantages and disadvantages. Also, users can easily produce publication quality output in R using Markdown (now Quarto) and LaTeX document presentation systems. In addition, R graphs can easily be imported into html, MS Word or LibreOffice documents.\nBy contrast with other statistical software, the R interface is minimal and consists of a terminal. Similar to languages such as Python or C, R users can access it via a programming interface or Integrated Development Environment (IDE). In this guide we will use R Studio, one of the most common IDEs for R.\nThe data used in this introduction is the British Social Attitudes Survey, 2017, Environment and Politics: Open Access Teaching Dataset, which can be downloaded from the UK Data Service website without registration. The website also has instructions on how to acquire and download large-scale survey datasets. Links and further information about the other training resources available online are provided at the end of this document.\n\nAlthough R has advantages over other statistical analysis software, it also has a few downsides, both of which are summarised below. Users should be reminded that as open-source software, R and its packages are mainly developed by volunteers, which makes it a very flexible and dynamic project, but at the same time reliant on developers’ free time and goodwill.\n\n\n\nAdvantages and downsides of R\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nR is free and allows users to perform almost any analysis they want.\nThe learning curve may be steep for users who do not have a prior background in statistics or programming.\n\n\nR puts statistical analysis closer to the reach of lay people rather than specialists.\n\n\n\n\n\n\n\nTransparency of use and programming of the software and its routines, which improves the peer-reviewing and quality control of the software.\n\n\n\nVery flexible.\nProblem solving (for both advanced users and beginners) may be time-consuming, depending on how common the problem encountered, and may lead to more time spent solving technical rather than substantive issues.\n\n\n\n\n\n\nAvailability of a wide range of advanced techniques not provided in other statistical software\nMany people who design R packages are, or will become busy academics. Packages can stop being maintained without notice.\n\n\n\n\n\n\nA very large user base provides abundant documentation, tutorials, and web pages.\n\n\n\n\n\n\nThere are several (sometimes many) ways of achieving a particular result in R. This can be confusing for novice researchers, but at the same time will allow users to tightly adjust their programmes to their needs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is R ?</span>"
    ]
  },
  {
    "objectID": "2_Using.html",
    "href": "2_Using.html",
    "title": "2  Using R: essential information",
    "section": "",
    "text": "2.1 Download and installation\nR can be downloaded for free from the CRAN website, installed and run like any other application. Versions for Windows, Mac and Linux are available. The standard and rather minimalist interface that appears when the programme is launched on Windows is shown below.\nThis interface merely allows the user to type in commands one by one in the console, and to install packages via pull-down menus. This setup is however rather minimal, not very ergonomic or user friendly. As with other statistical software, the preferred way of interacting with R for most users consist in writing code in separate files (also called scripts files) that are run whenever needed, which is not directly feasible with the standard R GUI.\nWriting R code and running script files are made easier via an Integrated Development Environment (IDE) such as RStudio for beginners to intermediate users Sublime Text, or the StatEt module for Eclipse for more advanced programmers. All are free, available for Windows, MacOS and Linux and offer users a large number of functionalities, such as syntax highlighting, integration with Github and Markdown/Quarto, and document previsualisation.\nSince RStudio has a large user base and is relatively easy for beginners, we will use it throughout this guide. However, in order for the guide to remain accessible to users who do not work with other IDEs, RStudio is used below as a mere interface to the R engine without relying on its advanced features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R: essential information</span>"
    ]
  },
  {
    "objectID": "2_Using.html#download-and-installation",
    "href": "2_Using.html#download-and-installation",
    "title": "2  Using R: essential information",
    "section": "",
    "text": "The standard R Windows interface",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R: essential information</span>"
    ]
  },
  {
    "objectID": "2_Using.html#installing-and-setting-up-rstudio",
    "href": "2_Using.html#installing-and-setting-up-rstudio",
    "title": "2  Using R: essential information",
    "section": "2.2 Installing and setting up RStudio",
    "text": "2.2 Installing and setting up RStudio\nRStudio needs to be installed separately from R. The program can be downloaded from the RStudio website. The site will automatically generate a link to the version most compatible with the computer used to access it. Once downloaded double click on the file and follow the installation instructions.\nBy default, the R Studio interface consists of four main panels, respectively known as the Script Editor (top left panel), the Console (bottom left panel), the Environment (top right panel) and the File/Directory/Help/Viewer (bottom right panel).\n\n\n\nThe R Studio default interface\n\n\nAs this rather complex interface can be visually overwhelming for some users and is not required for the purpose of this guide, we will minimise the Global Environment and Files/Directory/Help panels by clicking in the center of the window and dragging right to the edge of the screen. This way, only the script and console panels remain visible. The tiling of the panels can also be customised in Tools&gt;Global Options&gt;Pane Layout. For instance, Script can be moved to the bottom of the window and Console to the top:\n\n\n\nA customised R Studio interface",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R: essential information</span>"
    ]
  },
  {
    "objectID": "2_Using.html#interacting-with-r",
    "href": "2_Using.html#interacting-with-r",
    "title": "2  Using R: essential information",
    "section": "2.3 Interacting with R",
    "text": "2.3 Interacting with R\nAs already mentioned, one can type R commands directly in the console of RStudio and/or by writing sequences of commands in a script file.\nMost R commands – also known as functions in R jargon – adopt the following syntax:\n\n&gt; command(parameter1, parameter2, ...)\n\nAll R commands are followed by brackets, even if some of them take no parameters.\nIn the following example we are going to set up a default working directory, that is the default location where files will be opened from and saved to in the , by using the getwd() and setwd() commands. First, let us visualise the current default working directory.\n\ngetwd()\n\n[1] \"/home/mscsepw2/Documents/R_UKDS\"\n\n\nLet us say that all the files used in this guide arre going to be located in a folder called ‘R_UKDS’, inside ‘My Documents’. To tell R to use the folder ‘R_UKDS’ we type:\nFor Windows:\n&gt; setwd(\"C:/Documents and Settings/&lt;INSERT YOUR USERNAME HERE&gt;/My Documents/R_UKDS\")\nFor Mac:\n&gt; setwd(\"/Users/&lt;INSERT YOUR USERNAME HERE&gt;/Documents/R_UKDS\")\nFor Linux:\n\nsetwd(\"~/Documents/R_UKDS\")\n\nTyping getwd() confirms that the change has been recorded.\n\ngetwd()\n\n[1] \"/home/mscsepw2/Documents/R_UKDS\"\nNotes:\n\nAny character string that is neither a command or the name of an object (such as a variable name) needs to be put between inverted commas or quotation marks, otherwise it will be interpreted as the name of an object and R will return an error.\nsee the example below about loading user-created packages;\nEven when no parameters are specified for a command, brackets are compulsory as shown in the getwd() example above;\nR uses forward slashes rather than backslashes (unlike Windows applications, but like Linux) to separate directories. Using backlashes will return an error message;\nAlthough most R commands accept a large number of options to be specified. In many cases default values have been ‘factory set’ so that only the essential parameters need specifying.\n\nBeing object-oriented, the output of most R commands can be either directly displayed on the screen (as in the above example) or stored in objects that can be subsequently reused in further commands. This feature separates R from traditional statistical software.\nFor instance, typing:\n\na&lt;-getwd()\n\nwill store the output of getwd() (that is, the name of the current default directory) into an object called ‘a’. In order to view the content of a, one can just type its name:\n\na\n\n[1] \"/home/mscsepw2/Documents/R_UKDS\"\n\nWriting R scripts via R Studio\nMost users will want to write their code in a script file, similar to the ‘do’ file in Stata or syntax file in SPSS. R script files can be identified with their .R suffix. To open an existing R script in RStudio select File&gt;Open File (shortcut: Control+O) then the relevant script file. To create a new script select File&gt;New File&gt;R Script (shortcut: Control+Shift+N) this will open a new script window in which to type commands.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R: essential information</span>"
    ]
  },
  {
    "objectID": "2_Using.html#installing-and-loading-packages",
    "href": "2_Using.html#installing-and-loading-packages",
    "title": "2  Using R: essential information",
    "section": "2.4 Installing and loading packages",
    "text": "2.4 Installing and loading packages\nApart from a basic set of commands and functions, most of the tools offered by R are available in packages that need to be installed and downloaded separately from within R. This can also be done with the pull-down menus of RStudio.\nFor example, to install the ‘foreign’ package one need to type:\ninstall.packages(\"foreign\",repos = \"https://cloud.r-project.org\")\nIf the address of the package repository is not specified via the repos= option, a pull-down menu will appear, asking for one. Choosing https://cloud.r-project.org will automatically select the closest mirror site. Packages installation only needs to be done once.\nOriginally, foreign enabled users to import Stata (version 12 or older) or SPSS datasets. For Stata datasets saved under version 13 and above as well as SPSS datasets from version 16 onwards, the haven package is required.\ninstall.packages(\"haven\",repos = \"https://cloud.r-project.org\")\nTo use a package already installed in the local R library, the library() command is needed:\n\nlibrary(haven)\n\nSimply typing:\n&gt; library()\nWill list all packages installed on the computer that can be loaded in memory. This can be a rather long list!\n\nIn addition to the main archive of R packages, the CRAN website provides a series of manuals, including Writing R Extensions, which describes how users can write their own packages and submit them to CRAN.\nOnce a package is installed, it will be permanently stored in the local R library on the computer, unless it is deleted it with the remove.packages() command (this is not advised as this can break dependencies between packages!).\n&gt; remove.packages(\"name of the package\")\nPackages required for an analysis have to be loaded every time the programme is launched or a new R session is started (But not every time a syntax file is run!).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R: essential information</span>"
    ]
  },
  {
    "objectID": "2_Using.html#using-rs-internal-help-system",
    "href": "2_Using.html#using-rs-internal-help-system",
    "title": "2  Using R: essential information",
    "section": "2.5 Using R’s internal help system",
    "text": "2.5 Using R’s internal help system\nWithin R, the most straightforward way to request help with a command consists of a question mark followed by the command name, without a space in between. The standard help system in R (unless using RStudio or Eclipse) relies on the default web browser installed on your computer (ie Chrome, Firefox or Edge in most cases) to display pages.\nTyping:\n&gt; ?getwd \nIs equivalent of:\nhelp('getwd')\nand will open the help page for the getwd() command in the default web browser or the viewer tab of RStudio.\n\n\n\nHelp page for getwd()\n\n\nThis will work for any command directly available in the Base package that is loaded at startup or in other packages loaded via the library() command. Otherwise, R will return an error message.\nTyping two question marks followed by a keyword will search all of R for the available documentation for that keyword in the installed packages:\n\n??haven\n\n\n\n\nResults of help search for ‘haven’\n\n\nAn index of all commands and functions in the haven package can be obtained by typing:\n\nhelp(package='haven')\n\n\n\n\nHelp content index for the haven package\n\n\nNote: this command only works because the haven package was previously loaded in memory with the library() command.\nMore information about where to find help when using R is provided at the end of this document.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R: essential information</span>"
    ]
  },
  {
    "objectID": "2_Using.html#objects",
    "href": "2_Using.html#objects",
    "title": "2  Using R: essential information",
    "section": "2.6 Objects",
    "text": "2.6 Objects\nR is an object oriented language, which means that almost any information it processes is stored as ‘objects’ (i.e. containers) that can be manipulated independently. During an R session, multiple objects are available simultaneously (for instance datasets, but also summary tables or new variables produced from it). Typing:\n&gt; ls()\nwill list all the objects that are currently in memory.\nObjects belong to classes or types which have distinct properties. There are many classes of objects in R. By comparison, Stata has only macros, variables and scalars that are directly available to most users. Common object classes include:\n\nfactors: these are equivalent to categorical variables (see below);\nnumeric: numerical variables – whether continuous or ordinal;\ncharacter: alphanumeric variables;\nvectors: a list of items that are of the same type;\nlists: more complex list of objects, functions or datasets\ndata frames (datasets);\nmatrices, etc.\n\nSome operations are understandably only possible with certain types of objects: for example, mathematical functions can only be used with numeric objects.\nMore advanced users can also create their own object classes. Describing R objects and their properties is well beyond the purpose of this guide and users interested should consult the online documentation for further explanations.\nTo create or assign a value to an object, one uses the assignment operator (&lt;-). For example, we can assign the value 5 to an object called x.:\n\nx &lt;- 5  \n\nIf you type the letter x, the value ‘5’ will be returned in the console.\n\nx\n\n[1] 5\n\n\nThe object x will appear in the R environment after the ls() command.\n\nls()\n\n[1] \"x\"\n\n\n\nDeleting objects\nThe rm() function can be used to remove objects from the environment (session). These objects can be variables, lists, datasets, etc. For instance, to remove the object ‘x’, or the fictitious dataset called ‘mydata’:\n\nrm(x)   \nls()\n\ncharacter(0)\n\n\n&gt; rm(mydata)\n\n\n\nAmong the various classes of objects one may use in R, a few are essential to understand when analysing survey data. Their characteristics are briefly listed below;\n\n\nData frames\nData frames are typically the kind of objects in which survey datasets are stored. They are similar to datasets in traditional statistical software or worksheets in Microsoft Excel or LibreOffice Calc. They are objects that have indexed rows and columns, both of which may have names. Columns correspond to variables and lines or rows to observations. Any cell can be uniquely identified by its position.\nLet’s assume that we have a small data frame called ‘mydata’. Here are a few basic commands to examine it:\nDetermining the size of a data frame:\nthe dim() command returns the number of rows and columns of a data frame\n\ndim(mydata)\n\n[1] 50  6\n\n\nR tells us that our data is made of 50 rows and 6 columns, in other words of 50 observations and 6 variables.\nWhat if I wanted a quick overview of the dataset?\n\nhead(mydata)\n\n    RSex skipmeal                   Married\n1 Female       NA Married/living as married\n2 Female        1        Separated/divorced\n3 Female       NA Married/living as married\n4   Male       NA             Never married\n5   Male        1             Never married\n6   Male       NA Married/living as married\n        Poverty1                         HEdQual3\n1        Was not Higher educ below degree/A level\n2        Was not Higher educ below degree/A level\n3        Was not             O level or equiv/CSE\n4        Was not Higher educ below degree/A level\n5        Was not                 No qualification\n6 Was in poverty                             &lt;NA&gt;\n  NatFrEst\n1        5\n2       30\n3       50\n4       50\n5       50\n6       10\n\n\nThe head() command displays the first six lines of the dataset. Depending on the number of variables the output of head() can become quickly overwhelming, as the size of the lines on most screens is limited!\nObtaining the names of variables (or columns) in the dataset: This can be done using either ls() which we already have used, or the names() commands. ls() returns the variables names, sorted alphabetically, whereas names() returns them in their actual order in the data frame.\n\nls(mydata)\n\n[1] \"HEdQual3\" \"Married\"  \"NatFrEst\" \"Poverty1\"\n[5] \"RSex\"     \"skipmeal\"\n\nnames(mydata)\n\n[1] \"RSex\"     \"skipmeal\" \"Married\"  \"Poverty1\"\n[5] \"HEdQual3\" \"NatFrEst\"\n\n\nWe can see that in the data frame, the “RSex” column comes in fact before “Poverty1”.\nAccessing variables:\nVariables (or columns) of a data frame can be accessed by their name preceded by the $ sign. For example:\n\nmydata$NatFrEst\n\n [1]  5 30 50 50 50 10  1 NA  5 50 60 25 30  3 25 10 80\n[18]  5 50 50 40 45  2 82 60 40 10 10 40 15 30 30  2 10\n[35] 75 99 70 50 10 30  1 10 85 45 70 25 40 30 10 25\nattr(,\"value.labels\")\nnamed numeric(0)\n\n\nLists all values of NatFrEst, as well as additional technical information at the bottom.\nAlternatively, variables (columns)and observations (rows) can be identified numerically by their position in the data frame using square brackets:\ndataframe[row number,column number]\nGiven that RSex is the first column of our dataset, typing\n\nmydata[,1]\n\n [1] Female Female Female Male   Male   Male   Male  \n [8] Female Male   Male   Male   Female Female Male  \n[15] Female Female Female Female Female Female Male  \n[22] Female Female Male   Male   Female Female Female\n[29] Female Male   Female Female Female Female Male  \n[36] Male   Female Female Male   Female Male   Male  \n[43] Female Male   Female Male   Female Female Female\n[50] Male  \nLevels: Male Female\n\n\nReturns the same output as previously, that is all recorded observation of NaRfEst. Not specifying a row or column name within the square brackets tells R to display them all.\n\nmydata[6,]\n\n  RSex skipmeal                   Married\n6 Male       NA Married/living as married\n        Poverty1 HEdQual3 NatFrEst\n6 Was in poverty     &lt;NA&gt;       10\n\n\nReturns the values of all the variables for the sixth row of the data frame. Specifying both a row and column number, will return a unique observation:\n\nmydata[6,6]\n\n[1] 10\n\n\nwhich in this case is 10. Finally, more than one column or row can be displayed by concatenating their number using the c() function:\n\nmydata[c(6,9), 6]\n\n[1] 10  5\n\n\nThe above command returns respectively the sixth and 9th observations for the sixth column. Explicit columns names can be used instead of their number, provided that they are placed between inverted commas:\n\nmydata[c(6,9),'NatFrEst']\n\n[1] 10  5\n\n\nReturns the same result as above.\nWe can explore other types of objects commonly found in R using the same dataset. The type of a variable can be displayed by simply using the class() function.\n\n\nNumeric\nNumeric objects are simple numerical vectors (ie a single or a list of numbers). They can be standalone or part of a data frame. Such is for example the case of the variable NatFrEst, which measures the proportion of people making wrong benefits claims estimated by respondents .\n\nclass(mydata$NatFrEst)\n\n[1] \"numeric\"\n\n\n\n\nCharacter\nCharacter objects are alphanumeric vectors, that is variables which consist of text string(s).\n\nclass(mydata$Married)\n\n[1] \"character\"\n\nhead(mydata$Married) ### Let's look at the first five rows of Married\n\n[1] \"Married/living as married\"\n[2] \"Separated/divorced\"       \n[3] \"Married/living as married\"\n[4] \"Never married\"            \n[5] \"Never married\"            \n[6] \"Married/living as married\"\n\n\n\n\nFactors\nA distinctive feature of R is that categorical variables whether ordinal or polynomial are stored in objects known as factors. Factors should be thought of as a special type of variable with a discrete set of values, known as levels. Factors can be unordered or ordered. In our data, Rsex (Gender of the respondent) is such an object:\n\nclass(mydata$RSex)\n\n[1] \"factor\"\n\n\nThe main difference between factors and traditional categorical variables in Stata or SPSS is that they do not consist of arbitrary numerical values with which substantive value labels are associated.\nIt is always a good idea to check the ordering of factor levels in a newly created variable with the level() command\n\nlevels(mydata$RSex)\n\n[1] \"Male\"   \"Female\"\n\n\nreturns the levels of RSex. ‘Male’ is the first level of Rsex, and ‘Female’ the second one, irrespective of the values originally assigned and described in the codebook of the dataset.\nIt is possible to change the ordering of factor levels with the factor() function.\n\nmydata$RSex.n&lt;-factor(mydata$RSex, # We re-use the existing factor levels from Rsex\n                    levels = levels(mydata$RSex)[c(2,1)]) # and reorder them\n\nThe above code tells R to create a new factor variable -RSex.n - whose levels are identical to RSex, but with ‘Female’ (level number 2 in the original variable) coming first, and “Male” (level number 1) , second. The name of the new variable is arbitrary.\nWe can check the outcome:\n\nlevels(mydata$RSex.n)\n\n[1] \"Female\" \"Male\"  \n\n\nIn the next chapter an alternative approach to dealing with categorical variables imported from SPSS or Stata is presented.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R: essential information</span>"
    ]
  },
  {
    "objectID": "3_Opening.html",
    "href": "3_Opening.html",
    "title": "3  Opening datasets in R",
    "section": "",
    "text": "3.1 Importing files: essential information",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Opening datasets in R</span>"
    ]
  },
  {
    "objectID": "3_Opening.html#importing-files-essential-information",
    "href": "3_Opening.html#importing-files-essential-information",
    "title": "3  Opening datasets in R",
    "section": "",
    "text": "Spreadsheet and text files\nIn principle, any dataset whether in CSV, or spreadsheet format can be imported into R:\n\nCSV files can be directly imported with read.csv() from Base R.\nMS Excel spreadsheets can be opened with the read_excel() function from the readXL package or opened/written with read.xlsx() and write.xlsx() from the xlsx package.\nODS (Open Document Spreadheets) files from LibreOffice/OpenOffice Calc can be opened and written with read_ods()/write_ods() from the readODS package.\n\n\n\nSAS, SPSS, or Stata\nCurrently, the haven packages provides the most versatile and straightforward route to importing data from other mainstream statistical software. SPSS, Stata and SAS files can be opened with respectively read_spss(), read_dta() and read_sas(). The only potential downside is that it relies on ad hoc data formats and data structures for converting labelled categorical variables and attempts to mimic SPSS/Stata’s value and variable labels. More information is available here. We will be using haven throughout this guide.\nFor the record, the foreign package used to be the standard for importing MINITAB, SAS, SPSS and Stata datasets into R, but its development ceased in 2000 (Stata version 12).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Opening datasets in R</span>"
    ]
  },
  {
    "objectID": "3_Opening.html#the-2017-british-social-attitudes-survey",
    "href": "3_Opening.html#the-2017-british-social-attitudes-survey",
    "title": "3  Opening datasets in R",
    "section": "3.2 The 2017 British Social Attitudes Survey",
    "text": "3.2 The 2017 British Social Attitudes Survey\nIn the remainder of this guide, we will be using the British Social Attitudes Survey, 2017, Environment and Politics: Open Access Teaching Dataset, which can be downloaded from the UK Data Service website. We will import the SPSS version of the dataset, We will import the SPSS version of the dataset. We will assume that the data is saved in a folder named UKDA inside the Documents folder (on a Windows computer). We will set . C:\\\\Users\\\\Your_User_Name_Here\\\\Documents as the default working directory.\nThis way, we won’t have to specify the full path each time that we will be opening or saving file.\n\nsetwd(\"C:\\\\Users\\\\Your_User_Name_Here\\\\Documents\\\\UKDS\")\n\nWe can finally open the file:\n\nbsa&lt;-read_spss(\"8849spss_V1/bsa2017_open_enviropol.sav\"\n               )\n\nTyping:\n\nls()\n\n[1] \"bsa\"\n\n\nwill show us that the object ‘bsa’ has appeared in the environment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Opening datasets in R</span>"
    ]
  },
  {
    "objectID": "3_Opening.html#understanding-the-dataset",
    "href": "3_Opening.html#understanding-the-dataset",
    "title": "3  Opening datasets in R",
    "section": "3.3 Understanding the dataset",
    "text": "3.3 Understanding the dataset\nAs mentioned in earlier sections, we can find out the number of observations and variables in the dataset by typing the following:\n\ndim(bsa)\n\n[1] 3988   25\n\n\nWe can see that there are 3988 observations and 25 variables in the dataset.\nWhat if we want to get the list of all variables in the dataset? We need to type:\n\nls(bsa)\n\n [1] \"actchar\"          \"actpol\"           \"carallow\"         \"carenvdc\"        \n [5] \"carnod2\"          \"carreduc\"         \"cartaxhi\"         \"CCBELIEV\"        \n [9] \"ChildHh\"          \"eq_inc_quintiles\" \"govnosa2\"         \"HEdQual3\"        \n[13] \"leftrigh\"         \"libauth\"          \"Married\"          \"PartyId2\"        \n[17] \"plnenvt\"          \"plnuppri\"         \"Politics\"         \"RAgeCat\"         \n[21] \"RClassGp\"         \"Rsex\"             \"Sserial\"          \"Voted\"           \n[25] \"WtFactor\"        \n\n\nIf we want to get a better sense of the data, we use the head() function which will return the first six rows.\n\nhead(bsa)\n\n  Sserial Rsex RAgeCat Married ChildHh HEdQual3 eq_inc_quintiles RClassGp\n1  290001    1       3       1       1        3                3        4\n2  290002    2       7       2       2        2               NA        5\n3  290003    2       4       1       1        1                3        1\n4  290004    2       4       1       1        1                3        1\n5  290005    1       7       4       2        2                4        1\n6  290006    2       2       4       1        2                1        1\n  CCBELIEV carallow carreduc carnod2 cartaxhi carenvdc plnenvt plnuppri\n1        2       NA       NA      NA       NA       NA      NA       NA\n2        3       NA       NA      NA       NA       NA      NA       NA\n3       NA       NA       NA      NA       NA       NA      NA       NA\n4       NA       NA       NA      NA       NA       NA      NA       NA\n5       NA       NA       NA      NA       NA       NA      NA       NA\n6       NA       NA       NA      NA       NA       NA      NA       NA\n  Politics Voted actchar actpol govnosa2 PartyId2 leftrigh  libauth  WtFactor\n1        3     1       5      5        2        2      1.0 4.500000 0.9380587\n2        1     1      NA     NA        4        2      1.8 4.333333 0.6844214\n3        3     1      NA     NA       NA        2       NA       NA 0.9060821\n4        1     1       4      4        2        2      1.5 2.500000 1.3085513\n5        4     1      NA     NA       NA        3      2.0 3.166667 0.4392823\n6        4     1      NA     NA        3       NA      3.0 3.000000 0.5695720\n\n\nSingle variables may be also summarised with head().\nFor example:\n\nhead(bsa$Rsex)\n\n[1] Male   Female Female Female Male   Female\nLevels: skipped or na Male Female Dontknow Refusal\n\n\ndisplays the first six observations of Rsex (gender of respondents).\nSimply typing the name of a variable as in:\nbsa$Rsex\nwill list its first 1000 observations.\nOther commands, such as summary() provide more synthetic information.\n\nsummary(bsa$Rsex)\n\nskipped or na          Male        Female      Dontknow       Refusal \n            0          1806          2182             0             0 \n\n\nsummary() is a generic function that tailors the most appropriate output to an object class. Since Rsex is a categorical variable, the output of summary() is identical to what we would have obtained with the straightforward tabulation function table():\n\ntable(bsa$Rsex)\n\n\nskipped or na          Male        Female      Dontknow       Refusal \n            0          1806          2182             0             0 \n\n\nWhen encountering a numeric variable, summary() will compute basic descriptive statistics (mean, median, quartiles, maximum and minimum). For example, in the case of the libertarian-authoritarian scale libauth:\n\nsummary(bsa$libauth)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.167   3.000   3.500   3.511   4.000   5.000     775",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Opening datasets in R</span>"
    ]
  },
  {
    "objectID": "3_Opening.html#identifying-and-selecting-variables",
    "href": "3_Opening.html#identifying-and-selecting-variables",
    "title": "3  Opening datasets in R",
    "section": "3.4 Identifying and selecting variables",
    "text": "3.4 Identifying and selecting variables\nAs we have already seen, variables are objects. R automatically stores variables using the appropriate object class. Categorical variables are ‘Factors’ with ‘Levels’ as categories within these, while continuous variables are ‘Numeric’ types of objects. The class() displays the type of an object:.\n\nclass(bsa$Rsex)\n\n[1] \"factor\"\n\n\nThe levels() function returns the categories of the variable.\n\nlevels(bsa$Rsex)\n\n[1] \"skipped or na\" \"Male\"          \"Female\"        \"Dontknow\"     \n[5] \"Refusal\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Opening datasets in R</span>"
    ]
  },
  {
    "objectID": "3_Opening.html#factor-variables-in-the-haven-package",
    "href": "3_Opening.html#factor-variables-in-the-haven-package",
    "title": "3  Opening datasets in R",
    "section": "3.5 Factor variables in the haven package",
    "text": "3.5 Factor variables in the haven package\nAs we have seen in the previous chapter, statistical software such as SPSS or Stata treat categorical variables as numerical variables. Values labels are then ‘attached’, allocating a substantive meaning to these values. On the other hand, R records categorical variables in objects that are called factors, that may be ordered or not. Factors consist of a limited number of levels sequentially numbered values (ranging from 1 to the number of levels) that are paired with arbitrary character strings. The number of a factor level is therefore distinct from the values categorical variables are allocated in codebooks.\nUnfortunately, there are no straightforward ways to convert SPSS or Stata labelled categorical variables into R factors. The approach followed by the haven package is to preserve the arbitrary numeric values of the original variables, and add attributes that contain the value labels that can be manipulated separately. Attributes are a special type of R objects that have a name, and can be retrieved using the attr() function. haven-converted categorical variables all have a ‘label’ and ‘labels’ attribute. The former is the variable description aka variable label, the latter the value labels.\nLet’s examine the original variable description and value labels with the attr() function.\n\nattr(bsa$HEdQual3, \"label\")\n\n[1] \"Highest educational qual obtained - dv\"\n\n\nWe do the same with value labels:\n\nattr(bsa$HEdQual3, \"labels\")\n\n                          Degree Higher educ below degree/A level \n                               1                                2 \n            O level or equiv/CSE                 No qualification \n                               3                                4 \n                   DK/Refusal/NA \n                               8 \n\n\nThe value labels displayed above include both the text description and the original (i.e. codebook) numeric values from SPSS/Stata.\nThese haven-imported numeric variables can be converted into R factors using as_factor(), with their numeric values simply reflecting their order in the vector of levels. The factor levels in the converted variables can consist of either the value labels:\n\nlevels(\n       as_factor(bsa$HEdQual3, levels=\"labels\")\n       )\n\n[1] \"Degree\"                           \"Higher educ below degree/A level\"\n[3] \"O level or equiv/CSE\"             \"No qualification\"                \n[5] \"DK/Refusal/NA\"                   \n\n\n… or, as in the earlier output, of both the original SPSS/Stata numeric values and the value labels:\n\nlevels(\n       as_factor(bsa$HEdQual3, levels=\"both\")\n       )\n\n[1] \"[1] Degree\"                          \n[2] \"[2] Higher educ below degree/A level\"\n[3] \"[3] O level or equiv/CSE\"            \n[4] \"[4] No qualification\"                \n[5] \"[8] DK/Refusal/NA\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Opening datasets in R</span>"
    ]
  },
  {
    "objectID": "4_Manipulation.html",
    "href": "4_Manipulation.html",
    "title": "4  Essentials of Data Manipulation",
    "section": "",
    "text": "4.1 Creating and transforming numeric variables\nIn this section we will cover how to recode variables and deal with missing data.\nLet’s say we would like to transform our numerical political orientation variable: leftrigh into a logarithmic scale. We can use the log() function which is available in R Base and simply returns the natural logarithm (base-e). We will use the assignment operator ( &lt;- ) to create a new variable called ‘lnleftright’ from the original variable\nbsa$lnleftrigh &lt;- log(bsa$leftrigh)\nNote that if we had not specified bsa$ the command would have created a transformed variable outside of the BSA data frame. We can now check the results with summary()\nsummary(cbind(bsa$lnleftrigh,bsa$leftrigh))\n\n       V1               V2      \n Min.   :0.0000   Min.   :1.00  \n 1st Qu.:0.6931   1st Qu.:2.00  \n Median :0.8755   Median :2.40  \n Mean   :0.8707   Mean   :2.52  \n 3rd Qu.:1.0986   3rd Qu.:3.00  \n Max.   :1.6094   Max.   :5.00  \n NA's   :782      NA's   :782\nIt is not possible to pass several variables names directly to summary(). We need to group them first into a temporary object using cbind(). In the output V1 refers to the first variable, lnleftrigh.\nWe can easily create completely new variables in the dataset. For instance, the following will create test with a constant value of 1.\nbsa$test &lt;- 1\n\nsummary(bsa$test) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       1       1       1       1       1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Essentials of Data Manipulation</span>"
    ]
  },
  {
    "objectID": "4_Manipulation.html#recoding-variables",
    "href": "4_Manipulation.html#recoding-variables",
    "title": "4  Essentials of Data Manipulation",
    "section": "4.2 Recoding variables",
    "text": "4.2 Recoding variables\nRecoding categorical and numeric variables is very common in survey research. For example, let us create a dichotomic version of the marital status of BSA respondents. The original marital status is recorded by Married. Simply using summary will return descriptive statistics of its numeric values, which is not what we need here:\n\nsummary(bsa$Married)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.000   1.000   1.977   3.000   4.000       1 \n\n\nWhat we want instead is to work with the factor-converted version of Married.\n\nsummary(as_factor(bsa$Married))\n\nMarried/living as married        Separated/divorced                   Widowed \n                     2209                       530                       380 \n            Never married            No information                      NA's \n                      868                         0                         1 \n\n\nEven better, we can produce the exhaustive list of all of Married’s factor levels:\n\nlevels(as_factor(bsa$Married))\n\n[1] \"Married/living as married\" \"Separated/divorced\"       \n[3] \"Widowed\"                   \"Never married\"            \n[5] \"No information\"           \n\n\nWe can now move on to creating a new variable called Married2 where respondents are categorised into two new categories: ‘Not partnered’ and ‘Partnered’. The “separated/divorced” and ‘Never married’ categories of the ‘Married’ variable are recoded as ‘Not partnered’. It is always advised to create new variables when recoding old ones so the original data is not tampered with.\n\nbsa$Married.f&lt;-as_factor(bsa$Married,\"labels\")\nbsa$Married2 &lt;- ifelse(bsa$Married.f==\"Married/living as married\",\n                                      \"Partnered\",bsa$Married)\nbsa$Married2 &lt;- ifelse(bsa$Married.f==\"Widowed\" | \n                       bsa$Married.f==\"Never married\" | \n                      bsa$Married.f==\"Separated/divorced\",\n                                      \"Not partnered\",bsa$Married2)\n\nbsa$Married2&lt;-as.factor(bsa$Married2)\nlevels(bsa$Married2)\n\n[1] \"Not partnered\" \"Partnered\"    \n\nsummary(bsa$Married2)\n\nNot partnered     Partnered          NA's \n         1778          2209             1 \n\n\nThe second and fourth categories have been renamed to ‘Not partnered’. Now we have two levels: ‘Partnered’ and ‘Not partnered’\nifelse() is a convenient tool when it is required to work with Base R functions only or when variables have a limited number of categories. The syntax consists of three terms:\n\nthe condition to be evaluated: for example bsa$Married.f==\"Widowed\"\nwhat happens if the condition is met, for example the new variable takes the value “Not partnered”\nwhat happens if the condition is not met, for example, the new variable retains its existing value\n\nMore complex cases may require a more advanced function. The dplyr library provides a comprehensive set of data manipulation tools, such as case_when().\n\nlibrary(dplyr)\nbsa&lt;-bsa%&gt;%\n     mutate(Married3=case_when(\n               Married.f == \"Married/living as married\" ~ \"Partnered\",  \n               Married.f == \"Separated/divorced\" | \n               Married.f == \"Widowed\" ~ \"Not Partnered\",\n               Married.f == \"Never married\" ~ \"Not Partnered\"\n)\n)\nbsa$Married3&lt;-as.factor(bsa$Married3)\nsummary(bsa$Married3)\n\nNot Partnered     Partnered          NA's \n         1778          2209             1 \n\n\nThe syntax above created the Married3 variable, which is identical to Married2. Let’s decompose it:\n\ndplyr use the pipe symbol ie %&gt;% or |&gt; which enables to sequentially combine functions. We will come back to this later in this guide.\nmutate() is the generic variable creation/alteration command, and can handle complex combinations of conditions as well as multiple simultaneous variable creation operations.\ncase_when() is the function that allows recoding numerical, character, or factor variables. On the left hand side of the tilde ~ are the condition or the values that need to be matched in the original variable , and on the right hand side, the attributed ie recoded values in the new variable. Note that in this case, the recoded variable is by default a character object and needs to be converted into a factor for easier manipulation.\n\nExtra tips:\n\nAs with any data manipulation exercise, caution is required, and it is recommended to create new variables with the recoded value rather than alter an original variable when handling missing values.\nThe standard value attribution command in R is &lt;-. However, = will also work in many cases.\nUnless explicitly specified (in our case, by adding the bsa$ prefix to variable name), the objects created are not included in the data frame from which they were computed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Essentials of Data Manipulation</span>"
    ]
  },
  {
    "objectID": "4_Manipulation.html#missing-values",
    "href": "4_Manipulation.html#missing-values",
    "title": "4  Essentials of Data Manipulation",
    "section": "4.3 Missing Values",
    "text": "4.3 Missing Values\nExplicit or system missing values in R (i.e. values that R itself considers as missing) are represented as NA for factors and numerical variables. For character variables, missing values are simply empty strings, ie \"\". R has a series of functions specifically designed to handle NAs.\nR has fewer safety nets than other packages for handling missing values. Most functions won’t warn users about whether there are observations with missing values that have been dropped. On the other hand, some commands will return error messages and by default won’t run when missing values are present. This is the case of mean() for example.\n\n4.3.1 Inspecting missing data\nThe logical function is.na() assesses each observation in variables and identifies whether cases are valid or missing. The result will appear as a boolean TRUE/FALSE vector for each observation. is.na() can be combined with other functions:\n\nWith table() in order to get the frequencies of missing values of a specific variable.\nWith sum() in order to count the number of missing observations of variables or whole datasets.\n\n\ntable(                     # number of missing values in the leftright variable\n      is.na(bsa$leftrigh)\n      ) \n\n\nFALSE  TRUE \n 3206   782 \n\nsum(                       # of missing values in the whole dataset\n   is.na(bsa)\n   ) \n\n[1] 36593\n\nmean(                     # proportion of NAs for a variable\n     is.na(bsa$leftrigh)\n     ) \n\n[1] 0.1960883\n\nmean(                     # proportion of NAs in the dataset\n     is.na(bsa)\n     ) # returns the proportion in the dataset\n\n[1] 0.3058592\n\n\n\n\n4.3.2 Recoding missing values as NA (continuous variables)\nIt may sometimes be useful to recode implicit missing values (ie considered by the data producer as missing, but not by R) of either numeric objects or factors into &lt;NA&gt;, in order to simplify case selection when conducting analyses. This can either be done with Base R code or the more advanced data manipulation functions from the dplyr package that we explored earlier.\nLet’s assume for a moment that we would like to eliminate respondents aged under 25 for our analysis. A safe way to proceed is by creating a new dataset.\n\n# convert labelled numeric variable into factor for clarity\nbsa$RAgeCat.f &lt;- as_factor(bsa$RAgeCat) \ntable(bsa$RAgeCat.f)                      \n\n\n     18-24      25-34      35-44      45-54      55-59      60-64        65+ \n       223        591        650        729        320        333       1138 \nDK/Refused \n         0 \n\n# retains all values except those that match the condition:  \nbsa.adults&lt;-bsa[!bsa$RAgeCat.f==\"18-24\", ] \ntable(bsa.adults$RAgeCat.f)                      \n\n\n     18-24      25-34      35-44      45-54      55-59      60-64        65+ \n         0        591        650        729        320        333       1138 \nDK/Refused \n         0 \n\n\nWe can also notice that although there are now no observations left in the 18-24 category, it is still displayed by table(). This is because levels are attributes of factors and are not deleted with observations. We can remove unused levels permanently with droplevels()\n\nbsa.adults$RAgeCat.f&lt;-droplevels(bsa.adults$RAgeCat.f)\ntable(bsa.adults$RAgeCat.f)\n\n\n25-34 35-44 45-54 55-59 60-64   65+ \n  591   650   729   320   333  1138 \n\n\n\n\n4.3.3 Working with missing values\nExplicit missing values (coded as NA) can be taken care of by R’s own missing values functions. For instance using the na.rm=T or na.rm=TRUE option will remove missing values from an analysis (typing ?na.rm will provide more information). Below is a summary of how NAs are dealt with by common R commands:\n\n\n\nTreatment of missing values by R commands\n\n\n\n\n\n\n\nCommand\nDefault action\nParameter\n\n\n\n\nmean(), sd(),median()\nIncludes NA (may return an error)\nna.rm=T\n\n\ncor(),cov()\nIncludes NA (may return an error)\nuse=“complete.obs”\n\n\ntable()\nExcludes NA\nuseNA = “always” to display NAs\n\n\nxtabs()\nExcludes NA\naddNA = T to display NAs\n\n\nlm(),glm()\nExcludes NA\nna.action=NULL",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Essentials of Data Manipulation</span>"
    ]
  },
  {
    "objectID": "4_Manipulation.html#subsetting-datasets",
    "href": "4_Manipulation.html#subsetting-datasets",
    "title": "4  Essentials of Data Manipulation",
    "section": "4.4 Subsetting datasets",
    "text": "4.4 Subsetting datasets\nWhen analysing survey data. it is often necessary to limit the scope of computation to specific groups or subset of the data we may be interested in. There are many ways of subsetting datasets in R. We will review the most common here.\nUsing Base R\nMost subsetting commands involve some form of conditions whereby the characteristics of a subsample of interest are specified. Suppose we would like to examine the interest for politics among people aged 18-24.\nWe can either create an adhoc data frame:\n\ntable(bsa$Politics)\n\n\n   1    2    3    4    5 \n 739  982 1179  708  379 \n\nbsa.young&lt;-bsa[bsa$RAgeCat.f==\"18-24\",]  \ntable(bsa.young$Politics)\n\n\n 1  2  3  4  5 \n29 41 72 56 25 \n\n\nOr we can directly limit the extent of the analysis on the go:\n\n  table(\n        bsa$Politics[bsa$RAgeCat.f==\"18-24\"]\n        )\n\n\n 1  2  3  4  5 \n29 41 72 56 25 \n\n\nIn the first example it was necessary to include a comma after the condition. This is meant to indicate that we want to retain all variables ie columns in the dataset. The comma is not necessary in the second example as we are already working with a single variable.\nUsing dplyr*\nNow suppose we want to further restrict the analysis to people self-identifying as males. We could use the same Base R syntax as above, but with more than one condition coding tends to become a bit cumbersome. We could instead use the more convenient syntax from the dplyr package. Either:\n\nbsa.young.males&lt;-bsa%&gt;%\n  filter(RAgeCat.f==\"18-24\" & as_factor(Rsex)==\"Male\")  \n\ntable(as_factor(bsa.young.males$Politics))\n\n\nItem not applicable   ... a great deal,        quite a lot,               some, \n                  0                  15                  22                  30 \n     not very much,    or, none at all?          Don`t know             Refusal \n                 18                   9                   0                   0 \n\n\nOr as before, embed it as a condition within table() :\n\n  table(\n         as_factor(\n          bsa%&gt;%\n          filter(RAgeCat.f==\"18-24\" & as_factor(Rsex)==\"Male\")%&gt;%\n          select(Politics)\n  )\n  )\n\nPolitics\nItem not applicable   ... a great deal,        quite a lot,               some, \n                  0                  15                  22                  30 \n     not very much,    or, none at all?          Don`t know             Refusal \n                 18                   9                   0                   0 \n\n\nfilter() and select() are the functions that specify respectively rows and columns to be kept/removed. They can be combined or used independently and used in any order.\nWe are now equipped with the necessary information to move to the next stage and carry out basic analysis using R.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Essentials of Data Manipulation</span>"
    ]
  },
  {
    "objectID": "5_Descriptives.html",
    "href": "5_Descriptives.html",
    "title": "5  Descriptive statistics",
    "section": "",
    "text": "5.1 Continuous variables\nProducing descriptive statistics in R is relatively straightforward, as key functions are included by default in the Base package. We have already seen above that the summary() command provides essential information about a variable. For instance,\nsummary(bsa$leftrigh)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00    2.00    2.40    2.52    3.00    5.00     782\nprovides information about the mean, median and quartiles of the political scale of respondents.\nThe describe() command from the Hmisc package provides a more detailed set of summary statistics.\nlibrary(Hmisc)\ndescribe(bsa$leftrigh)\n\nError in proxy[, ..., drop = FALSE]: incorrect number of dimensions\nThe code above returns an error because describe() expects numeric values, and ‘leftrigh’ isn’t a pure numeric variable:\nclass(bsa$leftrigh)\n\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"\nIt is a numeric variable: ‘double’, but with some extra metadata, including haven-generated value and variable labels. In order for describe() to run properly, we need to convert leftrigh to Base R numeric format, either as a new variable or as shown below, temporarily:\ndescribe(as.numeric(bsa$leftrigh))\n\nas.numeric(bsa$leftrigh) \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n    3206      782       30    0.993     2.52      2.5   0.8831      1.2 \n     .10      .25      .50      .75      .90      .95 \n     1.4      2.0      2.4      3.0      3.6      4.0 \n\nlowest : 1    1.2  1.4  1.5  1.6 , highest: 4.4  4.6  4.75 4.8  5\ndescribe() also provides the number of observations (including missing and unique observations), deciles as well as the five largest and smallest values.\nCommands producing single statistics are also available:\nmean(bsa$leftrigh, na.rm = T)\n\n[1] 2.519911\n\nsd(bsa$leftrigh, na.rm = T)\n\n[1] 0.7852958\n\nmedian(bsa$leftrigh, na.rm = T)\n\n[1] 2.4\n\nmax(as.numeric(bsa$leftrigh), na.rm = T)\n\n[1] 5\n\nmin(as.numeric(bsa$leftrigh), na.rm = T)\n\n[1] 1\nAs previously, the na.rm = T option prevents missing values from being taken into account (in which case the output would have been NA, as this is the default behaviour of these functions). Similarly to describe() earlier, max() and min() need the variable to be converted into numeric format to deliver the desired output.\nWe could combine the output from the above commands into a single line using the c() function:\nc(\n  mean(bsa$leftrigh, na.rm = T),\n  sd(bsa$leftrigh, na.rm = T),\n  median(bsa$leftrigh, na.rm = T),\n  max(as.numeric(bsa$leftrigh), na.rm = T),\n  min(as.numeric(bsa$leftrigh), na.rm = T)\n)\n\n[1] 2.5199106 0.7852958 2.4000000 5.0000000 1.0000000\nUsing these individual commands may come in handy, for instance when further processing of the result is needed:\nm &lt;- mean(bsa$leftrigh, na.rm= T)\nLet’s round the results to two decimal places:\nrm &lt;- round(m,2)\nWe can see the final results by typing:\nrm\n\n[1] 2.52\nNote:\nround(mean(bsa$leftrigh,na.rm=T),2)\n\n[1] 2.52\nwould produce the same results using just one line of code .",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "5_Descriptives.html#bivariate-association-between-continuous-variables",
    "href": "5_Descriptives.html#bivariate-association-between-continuous-variables",
    "title": "5  Descriptive statistics",
    "section": "5.2 Bivariate association between continuous variables",
    "text": "5.2 Bivariate association between continuous variables\nThe Base R installation comes with a wide range of bivariate statistical functions. cor() and cov() provide basic measures of association between two variables. For instance, in order to measure the correlation between the left-right the libertarian-authoritarian scales:\n\ncor(bsa$leftrigh, bsa$libauth, use='complete.obs')\n\n[1] 0.009625928\n\n\nThe latter variable is records how far someone sits on the libertarian – authoritarian scale ranging from 1 to 5.\nA correlation of 0.009 indicates a positive but very small relationship. It can be interpreted as ’an increase in authoritarianism is associated with a marginal increase in rightwing views.\nNote: When using cor() and cov(), missing values are dealt with the use= option, which can either take “everything”, “all.obs”, “complete.obs”, “na.or.complete”, or “pairwise.complete.obs” values. See ?cor for additional information.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "5_Descriptives.html#categorical-variables",
    "href": "5_Descriptives.html#categorical-variables",
    "title": "5  Descriptive statistics",
    "section": "5.3 Categorical Variables",
    "text": "5.3 Categorical Variables\nAs with continuous variables, R offers several tools that can be used to describe the distribution of categorical variables. One- and two-way contingency tables are the most commonly used.\n\n5.3.1 One way frequency tables\nThere are several R commands that we can use to create frequency tables. The most common ones table(), xtabs() or ftable() which return the frequencies of observations within each level of a factor. For example, in order to obtain the political affiliation of BSA respondents in 2017:\n\ntable(as_factor(bsa$PartyId2))\n\n\n     Not applicable        Conservative              Labour    Liberal Democrat \n                  0                1263                1479                 241 \n        Other party                None         Green Party Other answer/DK/Ref \n                193                 515                  79                   0 \n\n\nAs with any other R functions, the outcome of table() can be stored as an object for further processing:\n\na&lt;-table(as_factor(bsa$PartyId2))\n\nIt is not directly possible to have proportions or percentages computed with table(). Proportions are obtained using the prop.table() function which in turn does not produce percentages. It is also a good idea to round the results for greater readability.\nEither:\n\nround(\n  100*\n    prop.table(a),\n  1) \n\n\n     Not applicable        Conservative              Labour    Liberal Democrat \n                0.0                33.5                39.2                 6.4 \n        Other party                None         Green Party Other answer/DK/Ref \n                5.1                13.7                 2.1                 0.0 \n\n\n… or:\n\nround(100*\n        prop.table(\n          table(as_factor(bsa$PartyId2))\n        ),\n      1)\n\n\n     Not applicable        Conservative              Labour    Liberal Democrat \n                0.0                33.5                39.2                 6.4 \n        Other party                None         Green Party Other answer/DK/Ref \n                5.1                13.7                 2.1                 0.0 \n\n\n\n\n5.3.2 Two way or more contingency table\nThe simplest way to produce a two-way contingency table is to pass a second variable to table():\n\ntable(as_factor(bsa$PartyId2), as_factor(bsa$Rsex))\n\n                     \n                      skipped or na Male Female Dontknow Refusal\n  Not applicable                  0    0      0        0       0\n  Conservative                    0  627    636        0       0\n  Labour                          0  644    835        0       0\n  Liberal Democrat                0  124    117        0       0\n  Other party                     0   97     96        0       0\n  None                            0  199    316        0       0\n  Green Party                     0   31     48        0       0\n  Other answer/DK/Ref             0    0      0        0       0\n\n\nBy default table does not discard empty factor levels - (i.e. categories with no observations), which may sometimes result in slightly cumbersome result. Using droplevels() on each variable resolves the issue:\n\ntable(droplevels(as_factor(bsa$PartyId2)), droplevels(as_factor(bsa$Rsex)))\n\n                  \n                   Male Female\n  Conservative      627    636\n  Labour            644    835\n  Liberal Democrat  124    117\n  Other party        97     96\n  None              199    316\n  Green Party        31     48\n\n\nHowever, when dealing with more than one variable it is recommended to use xtabs() instead as it has a number of desirable functions directly available as options. The syntax is slightly different as it relies on a formula – a R object consisting of elements separated by a tilde ‘~’. The variables to be tabulated are specified on the right hand side of the formula. In order to lighten the syntax, we will also recode PartyId2 and Rsex permanently into factors.\n\nbsa$PartyId2.f&lt;-as_factor(bsa$PartyId2)\nbsa$Rsex.f&lt;-as_factor(bsa$Rsex)\n\nxtabs(~PartyId2.f +Rsex.f,\n      data = bsa)\n\n                     Rsex.f\nPartyId2.f            skipped or na Male Female Dontknow Refusal\n  Not applicable                  0    0      0        0       0\n  Conservative                    0  627    636        0       0\n  Labour                          0  644    835        0       0\n  Liberal Democrat                0  124    117        0       0\n  Other party                     0   97     96        0       0\n  None                            0  199    316        0       0\n  Green Party                     0   31     48        0       0\n  Other answer/DK/Ref             0    0      0        0       0\n\n\nThe data= parameter does not have to be explicitly specified as simply using ´bsa’ will work. Other useful options are:\n\nsubset=, which allows direct specification of a subpopulation from which to derive the table;\ndrop.unused.levels=T to remove empty levels;\nweights~ variables on the right hand side of the formula will be treated as weights, a useful feature for survey analysis.\n\nAs previously prop.table() is necessary in order to obtain proportions:\n\nb&lt;-xtabs(~PartyId2.f +Rsex.f,\n         bsa,\n         drop.unused.levels = T)\n\nround(100*\n        prop.table(b),\n      1) ### Cell percentages\n\n                  Rsex.f\nPartyId2.f         Male Female\n  Conservative     16.6   16.9\n  Labour           17.1   22.1\n  Liberal Democrat  3.3    3.1\n  Other party       2.6    2.5\n  None              5.3    8.4\n  Green Party       0.8    1.3\n\n\nThe largest group in the sample (22.1%) is made of labour-voting females and the smallest, of green-voting males.\n\nround(100*\n        prop.table(b,1),\n      1) ### Option 1 for row percentages\n\n                  Rsex.f\nPartyId2.f         Male Female\n  Conservative     49.6   50.4\n  Labour           43.5   56.5\n  Liberal Democrat 51.5   48.5\n  Other party      50.3   49.7\n  None             38.6   61.4\n  Green Party      39.2   60.8\n\n\nConservative voters are more or less evenly split between men and women, whereas Labour and Green voters are more likely to be women.\n\nround(100*\n        prop.table(b,2),\n      1) ### Option 2 for column percentages\n\n                  Rsex.f\nPartyId2.f         Male Female\n  Conservative     36.4   31.1\n  Labour           37.4   40.8\n  Liberal Democrat  7.2    5.7\n  Other party       5.6    4.7\n  None             11.6   15.4\n  Green Party       1.8    2.3\n\n\nSimilar proportions of men voted Conservative and Labour (36-37%), whereas women were clearly more likely to vote Labour.\nThere is not a straightforward way to obtain percentages in three-way contingency tables with either xtabs() or table(). This is where ftable() function comes handy. For convenience, we converted RAgeCat into a factor.\n\nbsa$RAgeCat.f&lt;-as_factor(bsa$RAgeCat) \n\nround(100*\n        prop.table(\n          ftable(RAgeCat.f~PartyId2.f+Rsex.f,\n                 data=droplevels(bsa)\n                   )\n         ,1)\n      ,1) ### Option 1 for row,  2 for column percentages\n\n                        RAgeCat.f 18-24 25-34 35-44 45-54 55-59 60-64  65+\nPartyId2.f       Rsex.f                                                   \nConservative     Male               3.0   7.8  13.9  15.0   8.6   9.3 42.4\n                 Female             2.7   7.1   8.8  18.6   8.8   8.7 45.4\nLabour           Male               7.6  16.1  14.3  21.3   7.5   9.3 23.9\n                 Female             7.9  20.2  19.2  18.8   7.1   7.1 19.7\nLiberal Democrat Male               0.8  13.7  19.4  15.3   9.7   8.9 32.3\n                 Female             4.3   9.4  26.5   6.0   6.0   9.4 38.5\nOther party      Male               3.1  14.4  11.3  17.5  11.3  16.5 25.8\n                 Female             5.2  14.6  15.6  16.7  11.5   8.3 28.1\nNone             Male               7.5  22.1  20.6  17.1  11.1   6.0 15.6\n                 Female             8.5  22.5  20.6  21.8   7.3   6.3 13.0\nGreen Party      Male               6.5  32.3  16.1  29.0   6.5   3.2  6.5\n                 Female             6.2  18.8  25.0  20.8   6.2  10.4 12.5\n\n\nThe table gives the relative age breakdown for each gender/political affiliation combination (ie row percentages). Here again we used droplevels(): this removes unused factor levels which would otherwise be displayed and make the table difficult to read. droplevels() can be applied either to entire data frames or single variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "5_Descriptives.html#grouped-summary-statistics-for-continuous-variables",
    "href": "5_Descriptives.html#grouped-summary-statistics-for-continuous-variables",
    "title": "5  Descriptive statistics",
    "section": "5.4 Grouped summary statistics for continuous variables",
    "text": "5.4 Grouped summary statistics for continuous variables\nA common requirement in survey analysis is the ability to compare descriptive statistics across subgroups of the data. There are different ways to do this in R. We demonstrate below the most straightforward one, which is obtained by using some of the functions available in the dplyr package.\n\nbsa%&gt;%\n  group_by(PartyId2.f)%&gt;%\n  summarise(mdscore=median(libauth,na.rm=T),\n            sdscore=sd(libauth,na.rm=T))\n\n# A tibble: 7 × 3\n  PartyId2.f       mdscore sdscore\n  &lt;fct&gt;              &lt;dbl&gt;   &lt;dbl&gt;\n1 Conservative        3.67   0.587\n2 Labour              3.33   0.774\n3 Liberal Democrat    3.17   0.726\n4 Other party         3.67   0.739\n5 None                3.67   0.584\n6 Green Party         2.83   0.872\n7 &lt;NA&gt;                3.67   0.564\n\n\nThe above command produces a table of summary values (median and standard deviations) of the Liberal vs authoritarian scale. We can see from the first one that Green party voters are the most liberal, followed by Labour, whereas non voters and Conservatives are the most authoritarian. Liberal Democrats are the most cohesive group (i.e. with the smallest standard deviation). We chose to leave non-responses for PartyId2 for this analysis. Some users might want to remove them instead before computing their results as in the table below. We do this by using is.na(), which checks variables for the presence of system missing values, in conjunction with filter().\n\nbsa%&gt;%\n  filter(!is.na(PartyId2.f)) %&gt;%                              \n  group_by(Rsex.f,PartyId2.f) %&gt;%\n  summarise(mnscore=sd(libauth,na.rm=T),\n            mdscore=median(libauth,na.rm=T))\n\n# A tibble: 12 × 4\n# Groups:   Rsex.f [2]\n   Rsex.f PartyId2.f       mnscore mdscore\n   &lt;fct&gt;  &lt;fct&gt;              &lt;dbl&gt;   &lt;dbl&gt;\n 1 Male   Conservative       0.607    3.67\n 2 Male   Labour             0.765    3.33\n 3 Male   Liberal Democrat   0.766    3.17\n 4 Male   Other party        0.703    3.83\n 5 Male   None               0.616    3.67\n 6 Male   Green Party        1.04     2.67\n 7 Female Conservative       0.565    3.67\n 8 Female Labour             0.781    3.33\n 9 Female Liberal Democrat   0.688    3.17\n10 Female Other party        0.773    3.67\n11 Female None               0.565    3.67\n12 Female Green Party        0.744    3   \n\n\nWhen further broken down by gender, we can see that overall the same trends remain valid, with some nuances: male Green supporters are markedly more liberal than their female counterpart, the opposite being true among Conservative supporters.\nInstead of tables of summary statistics, we may want to have summary statistics computed as variables that will be added to the current dataset for each corresponding gender/political affiliation group. This is straightforward to do with dplyr, we just need to use the mutate() command.\n\nbsa&lt;-bsa%&gt;%\n  group_by(Rsex.f,PartyId2.f)%&gt;%\n  mutate(msscore=sd(libauth,na.rm=T),\n         mdscore=median(libauth,na.rm=T))\n\nHowever, we also need to add the newly created variables into the existing bsa dataset, which the first line of the syntax above does. We can check that the variables have been created and that the correct values have been assigned to each sex/affiliation category.\n\nnames(bsa)\n\n [1] \"Sserial\"          \"Rsex\"             \"RAgeCat\"          \"Married\"         \n [5] \"ChildHh\"          \"HEdQual3\"         \"eq_inc_quintiles\" \"RClassGp\"        \n [9] \"CCBELIEV\"         \"carallow\"         \"carreduc\"         \"carnod2\"         \n[13] \"cartaxhi\"         \"carenvdc\"         \"plnenvt\"          \"plnuppri\"        \n[17] \"Politics\"         \"Voted\"            \"actchar\"          \"actpol\"          \n[21] \"govnosa2\"         \"PartyId2\"         \"leftrigh\"         \"libauth\"         \n[25] \"WtFactor\"         \"PartyId2.f\"       \"Rsex.f\"           \"RAgeCat.f\"       \n[29] \"msscore\"          \"mdscore\"         \n\nbsa[4:8,c(\"Rsex\",\"PartyId2\",\"mdscore\")]\n\n# A tibble: 5 × 3\n  Rsex       PartyId2              mdscore\n  &lt;dbl+lbl&gt;  &lt;dbl+lbl&gt;               &lt;dbl&gt;\n1 2 [Female]  2 [Labour]              3.33\n2 1 [Male]    3 [Liberal Democrat]    3.17\n3 2 [Female] NA                       3.67\n4 1 [Male]    3 [Liberal Democrat]    3.17\n5 2 [Female]  6 [Green Party]         3",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "6_Weighted.html",
    "href": "6_Weighted.html",
    "title": "6  Producing weighted and survey design-informed estimates",
    "section": "",
    "text": "6.1 Central tendency and dispersion (continuous variables)\nMost users of social surveys will be looking to produce representative estimates when conducting analyses. Conducting such population inference entail using weights, which are meant to correct estimates for the under/over representation of certain groups in the sample due to the sampling process and non-response. Producing sound results relies not just on weighting estimates but also on computing adequate standard errors or confidence intervals, which measure the precision of the estimates.\nThe recommended approach to inferring confidence intervals and standard errors involves accounting for the survey design (ie the way sampling was carried out) when conducting analyses – which can be done with the survey package in R, a topic described in Section 6.3. At the same time, for users who are concerned with quickly computing reasonably accurate point estimates, rather than publication-quality results, it may be useful to be aware which common R commands and operations can be used with weights.\nThe stats package, part of Base R, includes weighted.mean() which, as indicated by its name, computes weighted estimates of the mean of a variable when weights are provided. However, the Hmisc package includes a more comprehensive set of functions that can be used when weighting estimates: wtd.mean(), wtd.var() and wtd.quantile(). The code below provides an illustration of weighted means, variance and median of the left-right political attitudes score used in previous chapters, each time comparing it with the unweighted estimates:\n### Mean\nc(mean(bsa$leftrigh,na.rm=T),\n  wtd.mean(bsa$leftrigh,bsa$WtFactor)\n  )\n\n[1] 2.519911 2.521589\n\n### Variance\nc(var(bsa$leftrigh,na.rm=T),\n  wtd.var(bsa$leftrigh,bsa$WtFactor))\n\n[1] 0.6166894 0.6195378\n\n### Median and quartiles\nc(quantile(bsa$leftrigh,na.rm=T,probs=c(.25,.5,.75)),\n  wtd.quantile(bsa$leftrigh,bsa$WtFactor,probs=c(.25,.5,.75)))\n\n25% 50% 75% 25% 50% 75% \n2.0 2.4 3.0 2.0 2.4 3.0\nThe above functions can be used in conjunction with group_by() and summarise() in order to compute weighted estimates of continuous variables by groups of categorical variables:\nbsa%&gt;%\n  filter(!is.na(RAgeCat.f))%&gt;%\n  group_by(RAgeCat.f)%&gt;%\n  summarise(Mean=wtd.mean(leftrigh,WtFactor),\n            Var=wtd.var(leftrigh,WtFactor),\n            Median=wtd.quantile(leftrigh,WtFactor,probs=c(.5))\n            )\n\n# A tibble: 7 × 4\n  RAgeCat.f  Mean   Var Median\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 18-24      2.49 0.556    2.4\n2 25-34      2.56 0.577    2.6\n3 35-44      2.52 0.615    2.4\n4 45-54      2.53 0.671    2.6\n5 55-59      2.54 0.653    2.4\n6 60-64      2.46 0.685    2.4\n7 65+        2.52 0.613    2.4",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Producing weighted and survey design-informed estimates</span>"
    ]
  },
  {
    "objectID": "6_Weighted.html#frequencies-and-contingency-tables",
    "href": "6_Weighted.html#frequencies-and-contingency-tables",
    "title": "6  Producing weighted and survey design-informed estimates",
    "section": "6.2 Frequencies and contingency tables",
    "text": "6.2 Frequencies and contingency tables\nNeither ftable() nor table() that were used in previous chapter allow weights. Although the Hmisc packages includes a wtd.table() function for one-way frequency tables, we recommend using xtabs() as previously, as it is more versatile and allows weights. Indeed, as variables used with xtabs() are specified on the right hand side of a formula:\n&gt; xtabs(~var1, data=mydata)\nor\n&gt; xtabs(~var1 + var2, data=mydata)\n… A variable containing weights can be passed to xtabs() by specifying its name on the left hand-side of the equation (or the tilde ~ )\n&gt; xtabs(weights~var1 + var2, data=mydata)\nLet’s apply this technique to investigate respondents’ agreement with the sentence: People should be able to travel by plane as much as they like, even if this harms the environment as recorded in the plnenvt variable.\n\nbsa$plnenvt.f&lt;-as_factor(bsa$plnenvt) # Converts the original variable into a factor\n \n## Unweighted vs weighted frequency tables\ncbind(\n  Unweighted=round(\n    100*prop.table(\n      xtabs(~plnenvt.f,bsa,\n            drop.unused.levels = T)\n        ),\n    1),\n  Weighted=round(\n    100*prop.table(\n      xtabs(WtFactor~plnenvt.f,bsa,\n            drop.unused.levels = T)\n      ),\n    1)\n)\n\n                           Unweighted Weighted\nagree strongly                    4.6      4.8\nagree                            16.0     15.9\nneither agree nor disagree       33.2     33.2\ndisagree                         36.3     37.0\ndisagree strongly                10.0      9.0\n\n\nObtaining a weighted contingency table of respondents’ views about flying by gender follow the same logic:\n\n## Unweighted vs weighted contingency tables\ncbind(\n  round(\n    100*prop.table(\n      xtabs(~plnenvt.f+Rsex.f,bsa,\n            drop.unused.levels = T),\n      1),\n    1),\n  round(\n    100*prop.table(\n      xtabs(WtFactor~plnenvt.f+Rsex.f,bsa,\n            drop.unused.levels = T),\n      1),\n    1)\n)\n\n                           Male Female Male Female\nagree strongly             50.0   50.0 46.1   53.9\nagree                      47.2   52.8 53.5   46.5\nneither agree nor disagree 40.8   59.2 43.6   56.4\ndisagree                   47.9   52.1 52.7   47.3\ndisagree strongly          42.3   57.7 41.5   58.5",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Producing weighted and survey design-informed estimates</span>"
    ]
  },
  {
    "objectID": "6_Weighted.html#inference-using-survey-procedures",
    "href": "6_Weighted.html#inference-using-survey-procedures",
    "title": "6  Producing weighted and survey design-informed estimates",
    "section": "6.3 Inference using survey procedures",
    "text": "6.3 Inference using survey procedures\nThe weighting procedures described above could be described as ‘quick and dirty’ in that they compute representative point estimates – i.e. single values. Computing the precision of survey data estimates – for example via their standard error – requires more than just adding weights to a command. Information about the survey design, its primary sampling units, strata and clusters is required so that robust standard errors, statistical tests and/or confidence interval can be computed. The Survey package was designed in order to deal with this set of issues. It provides functions for computing common estimates while accounting for the survey design. Its most important features are described below.\nIn order to use survey functions one first needs to create a svydesign object, in essence a version of the data that incorporates the sample design information available, then compute the required estimate using the svydesign object.\nA common issue with survey datasets available in the UK is that sampling information is often only available in a secured version of the data, restricting its access to authorised users in a secure lab. Although it is sometimes possible to use available variables to account for aspects of the sample design – region as a strata in the case of stratified samples – in most cases users are left with computing standard errors without sample design information, which amounts to assuming that the sample was drawn purely at random. Even if this is the case however, using the survey package is recommended, as it provides a coherent framework for computing survey parameters.\n\nlibrary(survey) ### Loading the package in memory\nbsa.design&lt;-svydesign(ids =~1,\n                      weights=~WtFactor,\n                      data=bsa) \n\nThe code above simply declares the survey design by creating the bsa.design object (the name is arbitrary). The ids= parameter is where primary sampling units are declared, as well as any clustering information as a formula ie ~PSU+cluster2id.... When PSU information is unavailable ids is given the value 1 or 0. A strata= and fpc= are available in order to declare the sampling strata and the variable used for finite population correction. None of these are available in the bsa dataset, and estimation commands will therefore rely on the assumption of simple random sampling.\nWe can now compute estimates comparable to those in the previous sections. The code below provides the mean of the left vs right political orientation indicator, as well as its 95% confidence interval:\n\na&lt;-svymean(~leftrigh,\n        bsa.design,\n        na.rm = T)### Computes the mean and its standard error...\n\na\n\n           mean     SE\nleftrigh 2.5216 0.0155\n\nconfint(a) ### ... and confidence interval\n\n            2.5 %   97.5 %\nleftrigh 2.491277 2.551902\n\n\nWe can similarly\n\noldsvyquantile(~leftrigh,\n            bsa.design,\n            quantiles=.5,\n            na.rm = T)\n\n         0.5\nleftrigh 2.4\n\n\nFrequency and contingency tables are computed using svytable(), whose syntax relies on formulas similarly to xtabs() in the previous chapter.\n\n### A frequency table...\nround(100*\n        prop.table(\n          svytable(~RAgeCat.f,bsa.design)\n        ),\n      1)\n\nRAgeCat.f\n18-24 25-34 35-44 45-54 55-59 60-64   65+ \n 11.2  17.2  16.1  17.9   7.9   6.8  22.8 \n\n### And a two-way contingency table:\n\nround(100*\n        prop.table(\n          svytable(~RAgeCat.f+Rsex.f,bsa.design),\n          1),\n      1)\n\n         Rsex.f\nRAgeCat.f Male Female\n    18-24 51.1   48.9\n    25-34 50.2   49.8\n    35-44 49.7   50.3\n    45-54 49.3   50.7\n    55-59 48.8   51.2\n    60-64 49.0   51.0\n    65+   45.4   54.6",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Producing weighted and survey design-informed estimates</span>"
    ]
  },
  {
    "objectID": "7_Graphs.html",
    "href": "7_Graphs.html",
    "title": "7  Graphs and plots",
    "section": "",
    "text": "7.1 Distributional graphs for continuous variables\nThere are two common ways to visualise data in R: either by using the straightforward but rather basic plotting commands from the Base package, or instead by delving into the more complex but much nicer looking functionalities of the ggplot package.\nPlots such as histograms or box plots are a convenient way to gain a quick overview of the distribution of a variable and are easy to produce. Going back to the BSA data, we can plot the distribution of left-right political orientations scores with the hist() command.\nhist(bsa$leftrigh,freq=FALSE)\nThe histogram should appear in the ‘Plot’ tab on the right hand side of the R Studio window. It shows us that political orientations are slightly skewed towards the left. The freq=FALSE option requires the y-axis to be expressed in terms of proportions rather than number of observations.\nTitles and labels can easily be added:\nhist(bsa$leftrigh,\n     freq=FALSE,\n     main=\"Histogram of political orientations, 2017\",\n     ylab=\"Proportions\",\n     xlab=\"Left-right political orientations score\")\nNote that main, ylab and xlab can be used with any Base R plot commands.\nWe can also produce a box and whisker plot of the same variable in order to get a better sense of the distribution of outliers:\nboxplot(bsa$leftrigh,\n        # main=\"Box and whisker plot of political orientations\",\n        ylab=\"Left-right political orientations score\"\n)\nThe generic plot() command produces scatterplots. Let’s try it with our left-right political orientations score, in conjunction with libauth, a libertarian vs authoritarian scale.\nplot(bsa$leftrigh,bsa$libauth) ### Scatterplot of left-right political orientations score\nThe scatterplot shows us that there is little association between the two variables. However, slightly fewer respondents simultaneously score high on the ‘authoritarianism’ and ‘right’ scales, perhaps unsurprisingly.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Graphs and plots</span>"
    ]
  },
  {
    "objectID": "7_Graphs.html#plotting-categorical-variables",
    "href": "7_Graphs.html#plotting-categorical-variables",
    "title": "7  Graphs and plots",
    "section": "7.2 Plotting categorical variables",
    "text": "7.2 Plotting categorical variables\nThe generic plot() function provides a quick way to produce bar plots of categorical data. For example, we can examine the distribution of political party affiliations (Politics variable). In order to do this, we convert it into a factor (i.e. categorical) variable as previously. Some preliminary abbreviation of the factor levels are also required in order for them to be displayed properly.\n\nbsa$PartyId2.f&lt;-droplevels( # Getting rid of unused factor levels for neater output\n  as_factor(bsa$PartyId2)   # Converting haven labelled variable to factor\n)\nlevels(bsa$PartyId2.f)&lt;-c(\n  \"Con\",\"Lab\",\"Lib Dems\",\"Other\",\"None\", \"Greens\" # Shorter level names\n  ) \n\nplot(bsa$PartyId2.f)\n\n\n\n\n\n\n\n\nMore advanced plots require the barplot() function, which can be used in conjunction with table(). Whereas table() creates the data that will be plotted, barplot() does the actual plotting. For instance, we can produce the same bar plot, but this time with percentages, by creating a frequency table as we did above in Section 5.2, then plot it.\n\nparty.tab&lt;-round(100*prop.table(\n  table(bsa$PartyId2.f)\n),\n1)\n\nparty.tab\n\n\n     Con      Lab Lib Dems    Other     None   Greens \n    33.5     39.2      6.4      5.1     13.7      2.1 \n\nbarplot(party.tab,\n        main=\"Political party affiliation\",\n        ylab=\"Percent\")\n\n\n\n\n\n\n\n\nWe can go further and create plots for two-way contingency tables of party affiliation by gender. This time we will do it in a single command:\n\n#t&lt;-xtabs(~PartyId2.f+Rsex.f,bsa)       # First, let's get the contingency table\nt&lt;-xtabs(~Rsex.f+PartyId2.f,bsa)       # First, let's get the contingency table\nbarplot(\n  round(100*\n          prop.table(t,2),              ## Column % (here, gender)\n    1),               ## Rounded to 1 decimal\n  beside = T,     ## Side-by-side bars   \n  main=\"Political party affiliation by gender\",\n  ylab=\"Percent\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Graphs and plots</span>"
    ]
  },
  {
    "objectID": "7_Graphs.html#more-advanced-plots",
    "href": "7_Graphs.html#more-advanced-plots",
    "title": "7  Graphs and plots",
    "section": "7.3 More advanced plots",
    "text": "7.3 More advanced plots\nSocial science research often requires more advanced plots than just bar charts in order to conduct insightful analyses, such as for instance comparing the mean or median value of a continuous outcome across two or more categorical variables. The ggplot package provides one of the most advanced set of tools for data visualisation currently available. A few examples are provided below.\n\nThree way barplots using ggplot2\nSay we would like to explore how differences in political party affiliations vary by gender and whether respondents have a degree-level education.\nLet us first prepare the data: we need to create the table of result, the proportion of degree vs non degree holders by gender and political party. This is a three-way contingency table, that we can obtain with ftable() as shown in Section 5.2, combined with prop.table() for the computation of proportions and round().\nAs they are more straightforward to handle in ggplot, we convert the table object created by ftable into a data frame. Although it is possible to specify titles and axis labels in the plotting command, we will keep things simple and have them already in the data.\nRather than using the full range of educational achievements recorded in HEdQual3, we would like instead to have a dichotomic variable measuring whether respondents are degree holders or not. Adding it directly in the ftable command as a boolean expression return a dichotomic variable: “TRUE” for Degree educated respondents, and “FALSE” for everyone else. We just need to change the levels of this factor variable to make them more intelligible. Finally, we change the variable names in our data frame.\n\nbsa$HEdQual3.f&lt;-droplevels(as_factor(bsa$HEdQual3))\npa&lt;-round(100*\n            prop.table(\n              ftable(bsa$PartyId2.f,bsa$Rsex.f,(bsa$HEdQual3.f==\"Degree\")\n                     ),\n              1),\n          1)\n\npa&lt;-data.frame(pa)\nlevels(pa$Var3)&lt;-c(\"Below\",\"Degree\")\nnames(pa)&lt;-c(\"Affiliation\",\"Gender\",\"Education\",\"Percent\")\npa\n\n   Affiliation Gender Education Percent\n1          Con   Male     Below    72.7\n2          Lab   Male     Below    70.1\n3     Lib Dems   Male     Below    50.0\n4        Other   Male     Below    86.5\n5         None   Male     Below    88.8\n6       Greens   Male     Below    43.3\n7          Con Female     Below    82.6\n8          Lab Female     Below    67.3\n9     Lib Dems Female     Below    54.4\n10       Other Female     Below    74.7\n11        None Female     Below    83.9\n12      Greens Female     Below    48.9\n13         Con   Male    Degree    27.3\n14         Lab   Male    Degree    29.9\n15    Lib Dems   Male    Degree    50.0\n16       Other   Male    Degree    13.5\n17        None   Male    Degree    11.2\n18      Greens   Male    Degree    56.7\n19         Con Female    Degree    17.4\n20         Lab Female    Degree    32.7\n21    Lib Dems Female    Degree    45.6\n22       Other Female    Degree    25.3\n23        None Female    Degree    16.1\n24      Greens Female    Degree    51.1\n\n\nWe are now ready to plot the data. ggplot() functions usually work as a succession of layers or options that are added to an initial plot specification. Each extra layer is added after a + sign. In the example below, we specify the data and the aesthetic (i.e. the basic parameters of the plot) with the first command: the x and y variables as well as the first grouping variable, in our case education). geom_bar() stipulates the bar plot, with the ṕosition=\"dodge\" option for the bars to be located side by side (position=“stack”would have them on top of each other). Finally, facet_wrap() splits the plot by gender.\n\nggplot(data=pa,aes(y=Percent,x=Affiliation,fill=Education))+\n  geom_bar(position=\"dodge\",stat=\"identity\")+\n  facet_wrap(~Gender)+\n  theme_minimal()+                  ### Theme for visualisation\n  scale_fill_manual(values=c(\"#702082\", \"#008755\"))+ ### Custom colours (optional)\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nMosaic plots\nMosaic plots provide a visualisation tool for two-way or more contingency tables. Table cells are plotted as rectangles whose surface is proportional to their overall number of observations whereas their length represents their frequency relative to that of the second variable for each category of the first one - this is equivalent to column percentages in a contingency table. In the example below, we are using the mosaic() function from the vcd package which provides both descriptive and model-based plots.\nThe basic parameters of a descriptive mosaic plot consist of:\n\nA R formula where the row and columns variables are specified;\nThe highlighting variable, for the display of relative percentage;\nBy default, the contrasting colours are shades of grey but can be customised, as can the direction of the bars.\n\nLabelling options are a bit mode arcane than Base R plot functions. They need to be specified within a labeling= labeling_border() statement. See help(labeling_border) for more detail. We used Varnames=F to hide the variable names from the plot, rot_labels, to specify that we did not want any rotation of the value labels, and offset_labels to prevent them to overlap with the rectangles.\n\nlibrary(vcd)                # Loading the vcd package\n\nmosaic(~PartyId2.f+Rsex.f,                            \n       data=bsa,                                      \n       highlighting = \"Rsex.f\",                       \n       labeling= labeling_border(varnames = F,            \n                                 rot_labels = c(0,0,0,0),\n                                 offset_labels = c(0, 0, 0, 1)\n                                ), # labelling functions from mosaic. See \n                                   # help(labeling_border) for more detail\n                    main=\"Mosaic plot of political party affiliation by gender\" \n              )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Graphs and plots</span>"
    ]
  },
  {
    "objectID": "8_Testing.html",
    "href": "8_Testing.html",
    "title": "8  Significance testing",
    "section": "",
    "text": "8.1 Differences between means\nThis section describes how to implement common statistical tests in R both without and with weights and survey design information. A working knowledge of these tests and their theoretical assumptions is assumed.\nTwo common ways of conducting significance testing consist in testing whether sample means significantly differ from 0 (one sample t-test), or between two groups (two samples t test). In the latter case, one can further distinguish between independent samples (where means come from different groups), or paired samples (when the same measure is taken at several points in time). Given that it is probably one of the most widely used statistical tests in social sciences, we will only cover the former here. Several R packages provide functions for conducting t tests. We will be using t.test(), provided by the stats package (R Base).\nSuppose we would like to test whether the libertarianism vs authoritarianism score libauth significantly differs between men and women using a t test. A two sided test is the default, with H_0 or the null hypothesis being that there are no differences between groups, and H_1 or the alternative hypothesis that the group means do indeed differ. The test is specified with a formula with on the left-hand side the quantity to be tested and on the right-hand side the grouping variable.\nOne sided tests can be conducted by specifying that the alternative hypothesis (H_1) is that quantities are either greater or smaller. t.test() assumes that by default the variances are unequal. This can be changed with the var.equal=T option.\n# Testing for significant differences in liberal vs authoritarian score\nsummary(\nt.test(libauth~Rsex.f,\n       data=bsa)\n)\n\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   1      -none- numeric  \np.value     1      -none- numeric  \nconf.int    2      -none- numeric  \nestimate    2      -none- numeric  \nnull.value  1      -none- numeric  \nstderr      1      -none- numeric  \nalternative 1      -none- character\nmethod      1      -none- character\ndata.name   1      -none- character\nNo significant differences (ie the difference in libauth between men and women is not significantly different from zero)\n# Testing for whether men have a lower (ie more left-wing)  score\nt.test(leftrigh~Rsex.f,\n       data=bsa, \n       alternative=\"less\")      \n\n\n    Welch Two Sample t-test\n\ndata:  leftrigh by Rsex.f\nt = -2.0687, df = 2858, p-value = 0.01933\nalternative hypothesis: true difference in means between group Male and group Female is less than 0\n95 percent confidence interval:\n        -Inf -0.01197607\nsample estimates:\n  mean in group Male mean in group Female \n            2.487564             2.546087\nMen have a significantly lower score on the scale (at the .05 threshold) and are therefore on average leaning more to the left than women.\nThe result of the above tests may be biased as they do not account for bias from either sample design or non-response. When results representative of the British population are required, a survey designed informed version of the t test should be used. The survey package that we used earlier in Chapter 6 provides such a function.\nlibrary(survey)\nbsa.design&lt;-svydesign(ids =~1,           # Declaring the survey design\n                      weights=~WtFactor,\n                      data=bsa) \n\nsvyttest(libauth~Rsex.f,bsa.design)   # SD informed t-test of libauth by gender\n\n\n    Design-based t-test\n\ndata:  libauth ~ Rsex.f\nt = 0.10069, df = 3211, p-value = 0.9198\nalternative hypothesis: true difference in mean is not equal to 0\n95 percent confidence interval:\n -0.05411275  0.05997165\nsample estimates:\ndifference in mean \n       0.002929454 \n\nsvyttest(leftrigh~Rsex.f,bsa.design) # SD informed t-test of leftrigh by gender\n\n\n    Design-based t-test\n\ndata:  leftrigh ~ Rsex.f\nt = 2.308, df = 3204, p-value = 0.02106\nalternative hypothesis: true difference in mean is not equal to 0\n95 percent confidence interval:\n 0.01085242 0.13337485\nsample estimates:\ndifference in mean \n        0.07211364\nIn this case the output of svyttest() did not lead to a different conclusion than the one we drew above. However, we can notice that the significance of differences in political affiliation has decreased.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "8_Testing.html#differences-in-variance",
    "href": "8_Testing.html#differences-in-variance",
    "title": "8  Significance testing",
    "section": "8.2 Differences in variance",
    "text": "8.2 Differences in variance\nAnother common significance test in social science is the variance test which consists of testing whether the variances of the same variable across two groups are equal. This is usually achieved by testing whether the ratio of the variance between the two groups is significantly different from zero. With the BSA data, this amounts to testing whether men and women are more homogeneous with regard to their political views.\nThe syntax for the variance test var.test() also included in stats is almost identical to that of t.test()\n\n# Testing for gender differences in liberal vs authoritarian score\nvar.test(libauth~Rsex.f,\n         data=bsa)\n\n\n    F test to compare two variances\n\ndata:  libauth by Rsex.f\nF = 1.0892, num df = 1434, denom df = 1777, p-value = 0.0879\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.9873927 1.2022204\nsample estimates:\nratio of variances \n          1.089239 \n\n\nSignificant differences in the variance between men and women was observed, but only at the .1 threshold.\n\n# Testing for whether men have a lower (ie more left-wing)  score\nvar.test(leftrigh~Rsex.f,\n         data=bsa,\n         alternative=\"greater\")      \n\n\n    F test to compare two variances\n\ndata:  leftrigh by Rsex.f\nF = 1.3218, num df = 1433, denom df = 1771, p-value = 1.263e-08\nalternative hypothesis: true ratio of variances is greater than 1\n95 percent confidence interval:\n 1.217167      Inf\nsample estimates:\nratio of variances \n            1.3218 \n\n\nThe variance of left-right political leaning is larger among men than women, in other words there are more divergence between men than between women.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "8_Testing.html#significance-of-measures-of-association",
    "href": "8_Testing.html#significance-of-measures-of-association",
    "title": "8  Significance testing",
    "section": "8.3 Significance of measures of association",
    "text": "8.3 Significance of measures of association\n\nBetween continuous variables\nAnother common statistical test in social science examines whether a coefficient of correlation is significantly different from 0 (alternative hypothesis).\n\ncor.test(bsa$leftrigh, bsa$libauth, \n         use='complete.obs')\n\n\n    Pearson's product-moment correlation\n\ndata:  bsa$leftrigh and bsa$libauth\nt = 0.54472, df = 3202, p-value = 0.586\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.02501074  0.04423951\nsample estimates:\n        cor \n0.009625928 \n\n\nAs we could have suspected, the correlation coefficient between the two scales is too small to be considered significantly different from zero.\n\n\nBetween categorical variables\nLet us go back to an earlier example, and test whether gender differences in political affiliations are due to chance or not using a chi-squared test of independence .\nThe chi-squared test is a very common test of association between categorical variables. It consists in examining whether a pattern of association between two variables is likely to be random or not, in other words whether the variability observed in a contingency table is significantly different from what could be expected were it due to chance.\nWe will be using chisq.test(), also from the stats package. By contrast with the test of correlation, the chisq.test() needs to be applied to contingency tables that have already been computed separately.\n\nt&lt;-xtabs(~PartyId2.f +Rsex.f,bsa)\nchisq.test(t)\n\n\n    Pearson's Chi-squared test\n\ndata:  t\nX-squared = 27.191, df = 5, p-value = 5.236e-05\n\n\nAs the R output shows, there are highly significant gender differences in political affiliations (p&lt;.001).\nDoes this remain true if we account for the survey design, as we did above for the t test? The survey package also has a survey design version of the chi square test:\n\nsvychisq(~PartyId2.f +Rsex.f, # We directly specify the contingency table here\n         bsa.design,        \n          statistic = \"Chisq\" # And we specify the kind of test we would like\n         )\n\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~PartyId2.f + Rsex.f, bsa.design, statistic = \"Chisq\")\nX-squared = 12.069, df = 5, p-value = 0.1126\n\n\nInterestingly this time, when accounting for survey design, sampling and non-response, gender differences in political affiliations are not significant anymore.\n\nVisualising association in contingency tables with mosaic plots\nIn the previous chapter we used mosaic plots for representing contingency tables of political party affiliation by gender. A nice feature of these plots is that they can also be used to visualise significant deviations between observed and expected values.\nThis relies on a function specified with the gp= option which defines the shading of the colours of the respective cells according to the size of these deviations fom expected values, also known as residuals. Thresholds for shading can be customised as required.\n\nmosaic(t,                            \n       shade=T,\n       gp = shading_hsv,               # shading function\n       labeling=labeling_border(\n         rot_labels = c(0,0,0,0),      # no label rotation on any plot facet\n         varname=F,                    # no  variable names on the plot\n         just_labels=\"left\",           # labels left justified\n         gp_labels=gpar(fontsize = 12),# label font size\n         offset_labels = c(0, 0, 0, 3) # margins between label and plot facet\n         ),\nmain = \"Unweighted mosaic plot of party affiliation by gender\" # Plot title\n         )\n\n\n\n\n\n\n\n\nThe red and blue shaded rectangles in the figure above denote respectively lower and higher numbers of observations than expected if the two variables were independent from each other.\nAnother convenient feature of mosaic() is that it readily accepts weighted contingency tables produced by the survey package. If we repeat what we did in Chapter 6, namely, declare the survey design, and perform a survey design-informed chi square test, we can then feed the weighted frequencies computed by svychisq into the mosaic plot.\n\nlibrary(survey) ### Loading the package in memory\nbsa.design&lt;-svydesign(ids =~1,\n                      weights=~WtFactor,\n                      data=bsa) \nt&lt;-svychisq(~PartyId2.f+Rsex.f,bsa.design,statistic = \"Chisq\")$observed\n\nThe plot below does not display shades of blue or red anymore, reflecting the fact that the weighted distributions of political party affiliation and gender are weakly associated.\n\nmosaic(t,                            \n       shade=T,\n       gp = shading_hcl,\n       labeling=labeling_border(\n         rot_labels = c(0,0,0,0),\n         varname=F,\n         just_labels=\"left\",\n         gp_labels=gpar(fontsize = 12),\n         offset_labels = c(0, 0, 0, 3)\n),\nmain = \"Weighted mosaic plot of party affiliation by gender\"\n\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "9_Regressions.html",
    "href": "9_Regressions.html",
    "title": "9  Regression analysis",
    "section": "",
    "text": "Regression is one of the most common modelling tools used in social science. The glm() function from Base R can be used for fitting linear and non-linear models. These include OLS regression, logistic/probit regression, and more generally any model falling under the Generalized Linear Model (GLM) framework.\nIn this section, we will use it to investigate the association between the level of education and whether someone voted or not at the last general elections. But let’s first briefly explore the variables using the class() and table():\n\nclass(bsa$HEdQual3.f)\n\n[1] \"factor\"\n\ntable(bsa$HEdQual3.f)\n\n\n                          Degree Higher educ below degree/A level \n                            1050                             1086 \n            O level or equiv/CSE                 No qualification \n                            1026                              747 \n\n\nand\n\nclass(bsa$Voted)\n\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"        \n\ntable(bsa$Voted)\n\n\n   1    2 \n2205  776 \n\n\nWe converted earlier HEdQual3 into a factor variable, but as we can see above, Voted is still a labelled numeric variable. It is a good idea to convert it to a factor as well. This is not absolutely necessary, but gives greater flexibility, for instance if we need to change the reference category on the go in the regression model.\n\nbsa$Voted.f&lt;-droplevels(as_factor(bsa$Voted)) # As before, factor conversion\nlevels(bsa$Voted.f)                           # Note that Yes comes before No\n\n[1] \"Yes\" \"No\" \n\n\nFor greater readability we can also shorten the levels of HEdQual3.f using the level() function:\n\nlevels(bsa$HEdQual3.f) ### The original level names  are a bit verbose...\n\n[1] \"Degree\"                           \"Higher educ below degree/A level\"\n[3] \"O level or equiv/CSE\"             \"No qualification\"                \n\n### ... We can shorten them easily\nlevels(bsa$HEdQual3.f) &lt;- c(\"Degree\",\"A level and above\",\"GCSE or equiv\",\"No Qual\")\n\ntable(bsa$HEdQual3.f)\n\n\n           Degree A level and above     GCSE or equiv           No Qual \n             1050              1086              1026               747 \n\n\nWhat about the levels for our dependent variable? By default, the first level of a factor will be used as the reference category. This can be also checked with the contrasts() function. In this case, 1 is associated with ‘No’, so the model will be predicting the probability of NOT voting. If the 1 was associated with ‘Yes’ then the model will be predicting the probability of voting.\n\ncontrasts(bsa$Voted.f)\n\n    No\nYes  0\nNo   1\n\n\nAs we are interested in the latter, we need to change the reference category using the relevel() function. We will create a new variable named Voted2 so as to keep the original variable intact.\n\n# Reverse the order\nbsa$Voted2 &lt;- relevel(bsa$Voted.f, ref = \"No\")\n\n# Check the contrasts\ncontrasts(bsa$Voted2)\n\n    Yes\nNo    0\nYes   1\n\n\nSince the outcome variable (Voted2) has a binomial distribution, we need to specify to the glm() function that we will be fitting a logistic regression model. We will do this by setting the argument family to ‘binomial’ and the link function to ‘logit’. We could also have used ‘probit’ instead as a link function. The code below runs the model and stores the result into an object called fit1:\n\nfit1 &lt;- glm(Voted2 ~ HEdQual3.f, \n            data=bsa, \n            family=binomial(link=logit)\n            )\n\nsummary(fit1)\n\n\nCall:\nglm(formula = Voted2 ~ HEdQual3.f, family = binomial(link = logit), \n    data = bsa)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  1.49561    0.09188  16.278  &lt; 2e-16 ***\nHEdQual3.fA level and above -0.21342    0.12514  -1.706   0.0881 .  \nHEdQual3.fGCSE or equiv     -0.64062    0.12191  -5.255 1.48e-07 ***\nHEdQual3.fNo Qual           -0.83672    0.12769  -6.553 5.65e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3297.6  on 2916  degrees of freedom\nResidual deviance: 3240.4  on 2913  degrees of freedom\n  (1071 observations deleted due to missingness)\nAIC: 3248.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nTo run a model controlling for gender Rsexand age RAgeCat, one simply needs to add them on the right-hand side of the formula, separated with a plus (+) sign.\n\nfit2 &lt;- glm(Voted2 ~ HEdQual3.f + Rsex.f + RAgeCat.f, \n            data=bsa, \n            family=binomial(link=logit)\n            )\n\nsummary(fit2)\n\n\nCall:\nglm(formula = Voted2 ~ HEdQual3.f + Rsex.f + RAgeCat.f, family = binomial(link = logit), \n    data = bsa)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  1.11251    0.20044   5.550 2.85e-08 ***\nHEdQual3.fA level and above -0.38676    0.13215  -2.927 0.003427 ** \nHEdQual3.fGCSE or equiv     -0.99023    0.13109  -7.554 4.23e-14 ***\nHEdQual3.fNo Qual           -1.90625    0.15687 -12.152  &lt; 2e-16 ***\nRsex.fFemale                -0.15708    0.09218  -1.704 0.088363 .  \nRAgeCat.f25-34              -0.24604    0.19670  -1.251 0.210996    \nRAgeCat.f35-44               0.20668    0.19808   1.043 0.296764    \nRAgeCat.f45-54               0.85685    0.20000   4.284 1.83e-05 ***\nRAgeCat.f55-59               0.84062    0.23225   3.619 0.000295 ***\nRAgeCat.f60-64               1.60276    0.25272   6.342 2.27e-10 ***\nRAgeCat.f65+                 2.16408    0.21450  10.089  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3293.1  on 2912  degrees of freedom\nResidual deviance: 2922.5  on 2902  degrees of freedom\n  (1075 observations deleted due to missingness)\nAIC: 2944.5\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nModel interpretation\nsummary() provides a broad overview of the model output, comparable to other statistical software. But fit1 and fit2 contain more information than what is displayed by summary(). For an overview, one can type:\n\nls(fit1)\n\n [1] \"aic\"               \"boundary\"          \"call\"             \n [4] \"coefficients\"      \"contrasts\"         \"control\"          \n [7] \"converged\"         \"data\"              \"deviance\"         \n[10] \"df.null\"           \"df.residual\"       \"effects\"          \n[13] \"family\"            \"fitted.values\"     \"formula\"          \n[16] \"iter\"              \"linear.predictors\" \"method\"           \n[19] \"model\"             \"na.action\"         \"null.deviance\"    \n[22] \"offset\"            \"prior.weights\"     \"qr\"               \n[25] \"R\"                 \"rank\"              \"residuals\"        \n[28] \"terms\"             \"weights\"           \"xlevels\"          \n[31] \"y\"                \n\n\n… Which displays a list of all the content items stored by it. We can request specific elements, the regression coefficients, by specifying its name following the $ sign:\n\nfit1$coefficients # extracting regression coefficients\n\n                (Intercept) HEdQual3.fA level and above \n                  1.4956126                  -0.2134240 \n    HEdQual3.fGCSE or equiv           HEdQual3.fNo Qual \n                 -0.6406223                  -0.8367243 \n\n\nShortcuts to some of these contents are available as functions such as coef() to extract the regression coefficients, or deviance() for the log-likelihood of the fitted model.\n\nls(fit2)\n\n [1] \"aic\"               \"boundary\"          \"call\"             \n [4] \"coefficients\"      \"contrasts\"         \"control\"          \n [7] \"converged\"         \"data\"              \"deviance\"         \n[10] \"df.null\"           \"df.residual\"       \"effects\"          \n[13] \"family\"            \"fitted.values\"     \"formula\"          \n[16] \"iter\"              \"linear.predictors\" \"method\"           \n[19] \"model\"             \"na.action\"         \"null.deviance\"    \n[22] \"offset\"            \"prior.weights\"     \"qr\"               \n[25] \"R\"                 \"rank\"              \"residuals\"        \n[28] \"terms\"             \"weights\"           \"xlevels\"          \n[31] \"y\"                \n\nround(fit2$coefficients,2)\n\n                (Intercept) HEdQual3.fA level and above \n                       1.11                       -0.39 \n    HEdQual3.fGCSE or equiv           HEdQual3.fNo Qual \n                      -0.99                       -1.91 \n               Rsex.fFemale              RAgeCat.f25-34 \n                      -0.16                       -0.25 \n             RAgeCat.f35-44              RAgeCat.f45-54 \n                       0.21                        0.86 \n             RAgeCat.f55-59              RAgeCat.f60-64 \n                       0.84                        1.60 \n               RAgeCat.f65+ \n                       2.16 \n\n### The coef() function will give the same output:\nround(coef(fit2),2)\n\n                (Intercept) HEdQual3.fA level and above \n                       1.11                       -0.39 \n    HEdQual3.fGCSE or equiv           HEdQual3.fNo Qual \n                      -0.99                       -1.91 \n               Rsex.fFemale              RAgeCat.f25-34 \n                      -0.16                       -0.25 \n             RAgeCat.f35-44              RAgeCat.f45-54 \n                       0.21                        0.86 \n             RAgeCat.f55-59              RAgeCat.f60-64 \n                       0.84                        1.60 \n               RAgeCat.f65+ \n                       2.16 \n\n\nIt is beyond the remit of this guide to describe the full output of glm(). See the stats package documentation for more detailed explanations.\n\n\nComputing odds ratios\nStandard logistic regression coefficients measure the logarithmic effect of variables on the probability of the outcome such as \\(log(\\beta_X)=P(y)\\). It is common practice to convert these into odd ratios by exponentiating them, such as that \\(\\beta_X=exp(P(y))\\).\nUsing the coef() and confint() functions, the code above respectively extracts the coefficients and the associated 95% confidence intervals from fit2 then collate them using cbind().\n\ncbind(\n  Beta_x=exp(\n    coef(fit2)\n    ),\n  exp(\n    confint(fit2)\n    )\n) \n\n                               Beta_x     2.5 %     97.5 %\n(Intercept)                 3.0419750 2.0618311  4.5276614\nHEdQual3.fA level and above 0.6792550 0.5237628  0.8795335\nHEdQual3.fGCSE or equiv     0.3714919 0.2868078  0.4796010\nHEdQual3.fNo Qual           0.1486366 0.1089442  0.2015607\nRsex.fFemale                0.8546343 0.7130680  1.0235515\nRAgeCat.f25-34              0.7818917 0.5300310  1.1469983\nRAgeCat.f35-44              1.2295882 0.8316608  1.8095457\nRAgeCat.f45-54              2.3557310 1.5886305  3.4827615\nRAgeCat.f55-59              2.3178122 1.4718049  3.6621685\nRAgeCat.f60-64              4.9667132 3.0428167  8.2090111\nRAgeCat.f65+                8.7065916 5.7183039 13.2696921\n\n\n\n\nPlotting regression coefficients\nWe can visualise the odd ratios and their confidence intervals using the plot.model() function from the sjPlot package. If not already present, sjPlot needs to be installed and loaded first.\ninstall.packages('sjPlot')\n\nlibrary(sjPlot)\nset_theme(base = theme_minimal()) ### Default sets of options \nplot_model(fit2,\n           colors = c(\"#702082\", \"#008755\") ### Added for better accessibility \n) \n\n\n\n\n\n\n\n\n\n\nAssessing model fit\nThe Akaike Information Criterion (AIC) is a measure of relative fit for maximum likelihood fitted models. It is used to compare the improvement in how several models fit some data relative to each other, allowing for the different number of parameters or degrees of freedom. The smaller the AIC, the better the fit. In order for the comparison to be valid, we need to ensure that the models were run with the same number of observations each time. As it is likely that the second model was run on a smaller sample, due to missing values for the Age and Sex variables, we will need to re-run the first one without these.\n\nfit1 &lt;- glm(Voted2 ~ HEdQual3.f, \n            data=bsa%&gt;%filter(!is.na(Rsex.f) & !is.na(RAgeCat.f)), \n            family=binomial(link=logit))\n\nc(fit1$aic,fit2$aic)\n\n[1] 3244.507 2944.535\n\n\nWe can see that the model controlling for gender and sex is a better fit to the data than the one without controls as it has an AIC of 2944.5 against 3244.5 for fit1.\nWith the information about the deviance from fit1 and fit2, we can also compute the overall significance of the model, that is whether the difference between the deviance (another likelihood-based measure of fit) for the fitted model is significantly different from that of the empty or null model. This is usually carried out by conducting a chi square test, accounting for the differences in the number of parameters (ie degrees of freedom) between the two models. As with other R code, this can be achieved step by step or in one go:\n\ndev.d&lt;-fit2$null.deviance - fit2$deviance # Difference in deviance\ndf.d&lt;-fit2$df.null - fit2$df.residual     # ... and degrees of freedom\np&lt;-1 - pchisq(dev.d, df.d)\nc(dev.d,df.d,p)\n\n[1] 370.5486  10.0000   0.0000\n\n\nThe Chi square test indicates that the difference in deviance of 370.5 with 10 degrees of freedom is highly significant (P&lt;.001)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "10_Further.html",
    "href": "10_Further.html",
    "title": "10  Further information",
    "section": "",
    "text": "Packages of interest\nThe following non exhaustive list provides a few examples of commands and packages that tackle common types of analysis which might be relevant to users of large scale social surveys:\n\nFurther regression analysis: the glm() command can be used for fitting a large number of regression models including Poisson and multinomial logistic regression. The packages lme4 and nlme include functions to fit respectively linear and non linear multilevel models, also known as mixed models.\nFor users interested in latent variable modelling the factanal() command from the stats package enables to conducts factor analysis. Other resources are available in the poLCA (Latent Class Analysis), ltm (Latent Trait modelling), sem (Structural Equation Modelling) packages. The Lavaan package also provides a wide range of functions for structural equation modelling including models with categorical outcomes.\nFor those conducting longitudinal and time series analysis the statsand the ‘tseries’ packages contains useful functions. The packages survival and eha deal with event history and survival analysis, whereas plm is designed for panel data and fixed and random effects models.\nUsing R for creating maps is now common among social scientists and geographers with packages such as rmaps, sp, rgdal, rgeos and ggmaps\n\n\n\nAdditional online resources\nThere are hundreds of web sites dedicated to R, in addition to CRAN and the main R help list, R-Help with its searchable archives. A few common ones are listed below:\n\nAs with other statistical packages, the UCLA and Princeton University websites provide a good starting point for beginners\nThe University of North Texas provides useful links to R resources\nThe R Bloggers website contains many posts about R.\nStackexchange is not specific to R but contains many forum-type questions and answers raised by R users\nThis website at Harding University presents useful information about basic plots in R.\nThis blog presents detailed tutorials for advanced data visualisation using ggplot2\nThe Centre for Multilevel modeling at Bristol University has several pages and training courses dedicated to R users interested in Multilevel modelling.\nThe UK Data Service has produced training material about creating maps in R, as part of an introduction to mapping crime data",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Further information</span>"
    ]
  },
  {
    "objectID": "11_References.html",
    "href": "11_References.html",
    "title": "11  References",
    "section": "",
    "text": "R Core Team. (2023). R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from https://www.r-project.org/ RStudio Team.\nRStudio: Integrated Development for R. Boston, USA: RStudio, Inc. Retrieved from http://www.rstudio.com/\nTennekes, M. (2018) tmap: Thematic Maps. R package version 1.10. Retrieved from https://cran.r-project.org/package=tmap\nWickham, H., & Francois, R. (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.4. Retrieved from https://cran.r-project.org/package=dplyr\nWickham, H. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Retrieved from https://cran.r-project.org/package=ggplot2",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>References</span>"
    ]
  }
]